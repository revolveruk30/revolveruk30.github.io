<html>
<title>NOTES</title>
<meta charset="UTF-8">
<meta name="robots" content="noindex,nofollow" />
<link rel="stylesheet" href="w3.css">
<link rel="stylesheet" href="w3-theme-black.css">
<script>
document.addEventListener("contextmenu", function(event){
event.preventDefault();
}, false);
</script>

<body>
<!-- Sidebar -->
<nav class="w3-sidebar w3-bar-block w3-collapse w3-small w3-theme-l5" id="mySidebar">
<BR>
<BR>
<p>
<a class="w3-button w3-hover-black" href="anderson.html"><u>Security Engineering, Ross Anderson</u></a><br>
<a class="w3-button w3-hover-black" href="bishop.html"><u>Introduction to Computer Security, Matt Bishop</u></a><br>
<a class="w3-button w3-hover-black" href="bellovin.html"><u>Thinking About Security, Steven Bellovin</u></a><br>
<a class="w3-button w3-hover-black" href="iracf.html"><u>Incident Response & Computer Forensics, Luttgens, Pepe, Mandia</u></a><br>
<a class="w3-button w3-hover-black" href="conklin.html"><u>Computer Security Principles, Conklin, White</u></a><br>

</nav>
<nav class="w3-searchbox" id="searchbox">
<input type="text" id="myInput" size="3" onkeyup="myFunction()" placeholder="Filter Standard by Keyword" title="Search">
<form method="GET" onsubmit="myHilitor.apply(hilite.value); return false;">
<input type="text" id="keywords" size="10" name="hilite" placeholder="Highlight Keywords">  
<input type="submit" value="Apply">
<input type="button" value="Remove" onclick="myHilitor.remove();">
</span>
</form>
</nav>
<div class="w3-main w3-theme-l5" style="margin-left:300px"> 
<div class="w3-row w3-padding-64">               
<h2 class="w3-text-teal"></h2>
<div id="playground">
<center>
<ul id="myUL">
Computer Security, Matt Bishop
<p>
<li><a href="#">Computer security is not just a science but also an art. It is an art because no system can be considered secure without an examination of how it is to be used. The definition of a secure computer necessitates a statement of requirements and an expression of those requirements in the form of authorized actions and authorized users. How will people, as well as other computers, interact with the computer system? How clear and restrictive an interface can a designer create without rendering the system unusable while trying to prevent unauthorized use or access to the data or resources on the system? Computer security is also a science. Its theory is based on mathematical constructions, analyses, and proofs. Its systems are built in accordance with the accepted practices of engineering. It uses inductive and deductive reasoning to examine the security of systems from key axioms and to discover underlying principles. These scientific principles can then be applied to untraditional situations and new theories, policies, and mechanisms.

<li><a href="#">Security mechanisms detect and prevent attacks and recover from those that succeed. Analyzing the security of a system requires an understanding of the mechanisms that enforce the security policy. It also requires a knowledge of the related assumptions and trust, which lead to the threats and the degree to which they may be realized. Such knowledge allows one to design better mechanisms and policies to neutralize these threats. This process leads to risk analysis. Human beings are the weakest link in the security mechanisms of any system. Therefore, policies and procedures must take people into account.

<li><a href="#">Confidentiality is the concealment of information or resources. The need for keeping information secret arises from the use of computers in sensitive fields such as government and industry. Access control mechanisms support confidentiality. One access control mechanism for preserving confidentiality is cryptography, which scrambles data to make it incomprehensible. A cryptographic key controls access to the unscrambled data, but then the cryptographic key itself becomes another datum to be protected. Confidentiality also applies to the existence of data, which is sometimes more revealing than the data itself. Access control mechanisms sometimes conceal the mere existence of data, lest the existence itself reveal information that should be protected.

<li><a href="#">Integrity refers to the trustworthiness of data or resources, and it is usually phrased in terms of preventing improper or unauthorized change. Integrity includes data integrity (the content of the information) and origin integrity (the source of the data, often called authentication). The source of the information may bear on its accuracy and credibility and on the trust that people place in the information. Integrity mechanisms fall into two classes: prevention mechanisms and detection mechanisms. Prevention mechanisms seek to maintain the integrity of the data by blocking any unauthorized attempts to change the data or any attempts to change the data in unauthorized ways. Detection mechanisms do not try to prevent violations of integrity; they simply report that the data's&#8364;'s integrity is no longer trustworthy. Integrity includes both the correctness and the trustworthiness of the data. The origin of the data (how and from whom it was obtained), how well the data was protected before it arrived at the current machine, and how well the data is protected on the current machine all affect the integrity of the data.

<li><a href="#">Availability refers to the ability to use the information or resource desired. Availability is an important aspect of reliability as well as of system design because an unavailable system is at least as bad as no system at all. The aspect of availability that is relevant to security is that someone may deliberately arrange to deny access to data or to a service by making it unavailable. Attempts to block availability, called denial of service attacks, can be the most difficult to detect because the analyst must determine if the unusual access patterns are attributable to deliberate manipulation of resources or of the environment. A deliberate attempt to make a resource unavailable may simply look like, or be an atypical event. In some environments, it may not even appear atypical.

<li><a href="#">Threats

<li><a href="#">A threat is a potential violation of security. The violation need not actually occur for there to be a threat. The fact that the violation might occur means that those actions that could cause it to occur must be guarded against (or prepared for). Shirey divides threats into four broad classes: disclosure, or unauthorized access to information; deception, or acceptance of false data; disruption, or interruption or prevention of correct operation; and usurpation, or unauthorized control of some part of a system.

<li><a href="#">Snooping , the unauthorized interception of information is a form of disclosure. It is passive, suggesting simply that some entity is listening to (or reading) communications or browsing through files or system information. Confidentiality services counter this threat.

<li><a href="#">Modification or alteration, an unauthorized change of information, covers three classes of threats. The goal may be deception, in which some entity relies on the modified data to determine which action to take, or in which incorrect information is accepted as correct and is released. If the modified data controls the operation of the system, the threats of disruption and usurpation arise. Unlike snooping, modification is active; it results from an entity changing information. Integrity services counter this threat.

<li><a href="#">Spoofing , an impersonation of one entity by another, is a form of both deception and usurpation. It lures a victim into believing that the entity with which it is communicating is a different entity. Integrity services (called authentication services in this context) counter this threat.

<li><a href="#">Denial of service , a long-term inhibition of service, is a form of usurpation, although it is often used with other mechanisms to deceive. The attacker prevents a server from providing a service. The denial may occur at the source (by preventing the server from obtaining the resources needed to perform its function), at the destination (by blocking the communications from the server), or along the intermediate path (by discarding messages from either the client or the server, or both). Availability mechanisms counter this threat.

<li><a href="#">Goals of Security

<li><a href="#">Critical to our study of security is the distinction between policy and mechanism. A security policy is a statement of what is, and what is not, allowed. A security mechanism is a method, tool, or procedure for enforcing a security policy. Mechanisms can be nontechnical, such as requiring proof of identity before changing a password; in fact, policies often require some procedural mechanisms that technology cannot enforce. Given a security policy's specification of secure and nonsecure actions, these security mechanisms can prevent the attack, detect the attack, or recover from the attack. The strategies may be used together or separately.

<li><a href="#">Prevention involves the implementation of mechanisms that users cannot override and that are trusted to be implemented in a correct, unalterable way so that the attacker cannot defeat the mechanism by changing it. Preventative mechanisms often are very cumbersome and interfere with system use to the point that they hinder normal use of the system. But some simple preventative mechanisms, such as passwords (which aim to prevent unauthorized users from accessing the system), have become widely accepted.

<li><a href="#">Detection is most useful when an attack cannot be prevented, but it can also indicate the effectiveness of preventative measures. Detection mechanisms accept that an attack will occur; the goal is to determine that an attack is underway, or has occurred, and report it. Typical detection mechanisms monitor various aspects of the system, looking for actions or information indicating an attack.

<li><a href="#">Recovery has two forms. The first is to stop an attack and to assess and repair any damage caused by that attack. The attacker may return, so recovery involves identification and fixing of the vulnerabilities used by the attacker to enter the system. In a second form of recovery, the system continues to function correctly while an attack is underway. This type of recovery is quite difficult to implement because of the complexity of computer systems. It draws on techniques of fault tolerance as well as techniques of security and is typically used in safety-critical systems. It differs from the first form of recovery because at no point does the system function incorrectly. However, the system may disable non-essential functionality.

<li><a href="#">A policy consists of a set of axioms that the policymakers believe can be enforced. Designers of policies always make two assumptions. First, the policy correctly and unambiguously partitions the set of system states into secure and nonsecure states. Second, the security mechanisms prevent the system from entering a nonsecure state. If either assumption is erroneous, the system will be nonsecure.

<li><a href="#">Trust cannot be quantified precisely. System specification, design, and implementation can provide a basis for determining how much to trust a system. This aspect of trust is called assurance. It is an attempt to provide a basis for bolstering (or substantiating or specifying) how much one can trust a system.

<li><a href="#">A specification is a (formal or informal) statement of the desired functioning of the system. It can be highly mathematical, using any of several languages defined for that purpose. It can also be informal, using, for example, English to describe what the system should do under certain conditions. The defining quality is a statement of what the system is allowed to do or what it is not allowed to do.

<li><a href="#">Risk Analysis

<li><a href="#">Any useful policy and mechanism must balance the benefits of the protection against the cost of designing, implementing, and using the mechanism. This balance can be determined by analyzing the risks of a security breach and the likelihood of it occurring. To determine whether an asset should be protected, and to what level, requires analysis of the potential threats against that asset and the likelihood that they will materialize. The level of protection is a function of the probability of an attack occurring and the effects of the attack should it succeed. If an attack is unlikely, protecting against it has a lower priority than protecting against a likely one.

<li><a href="#">Risk is a function of environment. Attackers from a foreign country are not a threat to the company when the computer is not connected to the Internet. Risks change with time. If a company's network is not connected to the Internet, there seems to be no risk of attacks from other hosts on the Internet. However, despite any policies to the contrary, someone could connect a modem to one of the company computers and connect to the Internet through the modem. Should this happen, any risk analysis predicated on isolation from the Internet would no longer be accurate.

<li><a href="#">Regardless of the strength of the technical controls, if nontechnical considerations affect their implementation and use, the effect on security can be severe. Moreover, if configured or used incorrectly, even the best security control is useless at best and dangerous at worst. Thus, the designers, implementers, and maintainers of security controls are essential to the correct operation of those controls.

<li><a href="#">People who have some motive to attack an organization and are not authorized to use that organization's systems are called outsiders and can pose a serious threat. Experts agree, however, that a far more dangerous threat comes from disgruntled employees and other insiders who are authorized to use the computers. Insiders typically know the organization of the company's systems and what procedures the operators and users follow and often know enough passwords to bypass many security controls that would detect an attack launched by an outsider. Insider misuse of authorized privileges is a very difficult problem to solve.

<li><a href="#">Untrained personnel also pose a threat to system security. System administrators who misread the output of security mechanisms, or do not analyze that output, contribute to the probability of successful attacks against their systems. Similarly, administrators who misconfigure security-related features of a system can weaken the site security. Users can also weaken site security by misusing security mechanisms (such as selecting passwords that are easy to guess).

<li><a href="#">Lack of training need not be in the technical arena. Many successful break-ins have arisen from the art of social engineering. If operators will change passwords based on telephone requests, all an attacker needs to do is to determine the name of someone who uses the computer.

<li><a href="#">Underlying computer security are key assumptions describing what the site and the system accept as true or trustworthy; understanding these assumptions is the key to analyzing the strength of the system's security. This notion of trust is the central notion for computer security. If trust is well placed, any system can be made acceptably secure. If it is misplaced, the system cannot be secure in any sense of the word.

<li><a href="#">Access Control Matrix

<li><a href="#">A protection system describes the conditions under which a system is secure. The state of a system is the collection of the current values of all memory locations, all secondary storage, and all registers and other components of the system. The subset of this collection that deals with protection is the protection state of the system. An access control matrix is one tool that can describe the current protection state.

<li><a href="#">Characterizing a secure system is the function of a security policy; preventing the system from entering an insecure state is the function of a security mechanism. The access control matrix model is the most precise model used to describe a protection state. It characterizes the rights of each subject (active entity, such as a process) with respect to every other entity.

<li><a href="#">As the system changes, the protection state changes. When a command changes the state of the system, a state transition occurs. In practice, any operation on a real system causes multiple state transitions; the reading, loading, altering, and execution of any datum or instruction causes a transition. We are concerned only with those state transitions that affect the protection state of the system, so only transitions that alter the actions a subject is authorized to take are relevant. The access control matrix model is an abstract model of the protection state, and when one talks about the meaning of some particular access control matrix, one must always talk with respect to a particular implementation or system.

<li><a href="#">The own right is a distinguished right. In most systems, the creator of an object has special privileges: the ability to add and delete rights for other users (and for the owner). When a process accesses a directory, read means to be able to list the contents of the directory; write means to be able to create, rename, or delete files or subdirectories in that directory; and execute means to be able to access files or subdirectories in that directory. When a process accesses another process, read means to be able to receive signals, write means to be able to send signals, and execute means to be able to execute the process as a subprocess.

<li><a href="#">The access control matrix is the primary abstraction mechanism in computer security. In its purest form, it can express any expressible security policy. In practice, it is not used directly because of space requirements; most systems have (at least) thousands of objects and could have thousands of subjects, and the storage requirements would simply be too much. However, its simplicity makes it ideal for theoretical analyses of security problems.

<li><a href="#">Security Policies

<li><a href="#">A security policy defines secure for a system or a set of systems. Security policies can be informal or highly mathematical in nature. After defining a security policy precisely, we expand on the nature of trust and its relationship to security policies. A security policy is a statement that partitions the states of the system into a set of authorized, or secure, states and a set of unauthorized, or nonsecure, states. A secure system is a system that starts in an authorized state and cannot enter an unauthorized state. A breach of security occurs when a system enters an unauthorized state.

<li><a href="#">A security policy considers all relevant aspects of confidentiality, integrity, and availability. With respect to confidentiality, it identifies those states in which information leaks to those not authorized to receive it. This includes not only the leakage of rights but also the illicit transmission of information without leakage of rights, called information flow. With respect to integrity, a security policy identifies authorized ways in which information may be altered and entities authorized to alter it. Authorization may derive from a variety of relationships, and external influences may constrain it. With respect to availability, a security policy describes what services must be provided. It may present parameters within which the services will be accessible. It may require a level of service, for example, that a server will provide authentication data within 1 minute of the request being made. This relates directly to issues of quality of service.

<li><a href="#">A military security policy (also called a governmental security policy) is a security policy developed primarily to provide confidentiality. The name comes from the military's need to keep information, such as the date that a troop ship will sail, secret. Although integrity and availability are important, organizations using this class of policies can overcome the loss of either. But the compromise of confidentiality would be catastrophic because an opponent would be able to plan countermeasures (and the organization may not know of the compromise). Confidentiality is one of the factors of privacy, an issue recognized in the laws of many government entities (such as the Privacy Act of the United States).

<li><a href="#">A commercial security policy is a security policy developed primarily to provide integrity. The name comes from the need of commercial firms to prevent tampering with their data because they could not survive such compromises. If the integrity of the computer holding the accounts were compromised, the balances in the customers' accounts could be altered, with financially ruinous effects.

<li><a href="#">A system administrator receives a security patch for her computer's operating system. She installs it. Has she improved the security of her system? She has indeed, given the correctness of certain assumptions: She is assuming that the patch came from the vendor and was not tampered with in transit. She is assuming that the vendor tested the patch thoroughly. She is assuming that the vendor's test environment corresponds to her environment. Otherwise, the patch may not work as expected. She is assuming that the patch is installed correctly. These assumptions are fairly high-level, but invalidating any of them makes the patch a potential security problem. Any security policy, mechanism, or procedure is based on assumptions that, if incorrect, destroy the superstructure on which it is built. Analysts and designers (and users) must bear this in mind because unless they understand what the security policy, mechanism, or procedure is based on, they jump from an unwarranted assumption to an erroneous conclusion.

<li><a href="#">A security policy may use two types of access controls, alone or in combination. In one, access control is left to the discretion of the owner. In the other, the operating system controls access, and the owner cannot override the controls.

<li><a href="#">If an individual user can set an access control mechanism to allow or deny access to an object, that mechanism is a discretionary access control (DAC), also called an identity-based access control (IBAC). Discretionary access controls base access rights on the identity of the subject and the identity of the object involved. Identity is the key; the owner of the object constrains who can access it by allowing only particular subjects to have access. The owner states the constraint in terms of the identity of the subject, or the owner of the subject.

<li><a href="#">When a system mechanism controls access to an object and an individual user cannot alter that access, the control is a mandatory access control (MAC), occasionally called a rule-based access control. The operating system enforces mandatory access controls. Neither the subject nor the owner of the object can determine whether access is granted. Typically, the system mechanism will check information associated with both the subject and the object to determine whether the subject should access the object. Rules describe the conditions under which access is allowed.

<li><a href="#">An originator controlled access control (ORCON or ORGCON) bases access on the creator of an object (or the information it contains). The goal of this control is to allow the originator of the file (or of the information it contains) to control the dissemination of the information. The owner of the file has no control over who may access the file.

<li><a href="#">Trust underlies all policies and enforcement mechanisms. Policies themselves make assumptions about the way systems, software, hardware, and people behave. At a lower level, security mechanisms and procedures also make such assumptions. Even when rigorous methodologies are applied, the methodologies themselves simply push the assumptions, and therefore the trust, to a lower level. Understanding the assumptions and the trust involved in any policies and mechanisms deepens one's understanding of the security of a system.

<li><a href="#">Confidentiality Policies

<li><a href="#">A confidentiality policy, also called an information flow policy, prevents the unauthorized disclosure of information. Unauthorized alteration of information is secondary. The simplest type of confidentiality classification is a set of security clearances arranged in a linear (total) ordering. These clearances represent sensitivity levels. The higher the security clearance, the more sensitive the information (and the greater the need to keep it confidential). A subject has a security clearance. An object has a security classification.

<li><a href="#">The goal of the Bell-LaPadula security model is to prevent read access to objects at a security classification higher than the subject's clearance. We say that subjects have clearance at (or are cleared into, or are in) a security level and that objects are at the level of (or are in) a security level.

<li><a href="#">According to the Simple Security Condition, S can read O if and only if the security clearance of the object is less than or equal to the security clearance of the subject and S has discretionary read access to O. The simple security condition is often described as no read up.

<li><a href="#">According to the *-Property Security Condition, S can write O if and only if the security clearance of the subject is less than or equal to the security clearance of the object and S has discretionary write access to O. The *-Property Security Condition is often described as no write down.

<li><a href="#">Security levels change access. Because categories are based on a need to know someone with access to the category set A presumably has no need to access items in the category B. Hence, read access should be denied, even if the security clearance of the subject is higher than the security classification of the object.

<li><a href="#">At times, a subject must communicate with another subject at a lower level. This requires the higher-level subject to write into a lower-level object that the lower-level subject can read. The model provides a mechanism for allowing this type of communication. A subject has a maximum security level and a current security level. The maximum security level must dominate the current security level. A subject may (effectively) decrease its security level from the maximum in order to communicate with entities at lower security levels.

<li><a href="#">Integrity Policies

<li><a href="#">Commercial requirements differ from military requirements in their emphasis on preserving data integrity. These requirements suggest several principles of operation. Separation of duty. The principle of separation of duty states that if two or more steps are required to perform a critical function, at least two different people should perform the steps. Separation of function. Developers do not develop new programs on production systems because of the potential threat to production data. Auditing. Commercial systems emphasize recovery and accountability. Auditing is the process of analyzing systems to determine what actions took place and who performed them.

<li><a href="#">In 1977, Biba studied the nature of the integrity of systems. In his model, a system consists of a set S of subjects, a set O of objects, and a set I of integrity levels. The levels are ordered. Data at a higher level is more accurate and/or reliable (with respect to some metric) than data at a lower level. Again, this model implicitly incorporates the notion of trust; in fact, the term trustworthiness is used as a measure of integrity level. Integrity labels, in general, are not also security labels. They are assigned and maintained separately because the reasons behind the labels are different. Security labels primarily limit the flow of information; integrity labels primarily inhibit the modification of information.

<li><a href="#">In 1987, David Clark and David Wilson developed an integrity model radically different from previous models. This model uses transactions as the basic operation, which models many commercial systems more realistically than previous models. One main concern of a commercial environment, as discussed above, is the integrity of the data in the system and of the actions performed on that data. The data is said to be in a consistent state (or consistent) if it satisfies given properties. e.g. If money is deposited in a bank account the balance should change accordingly.

<li><a href="#">Someone must certify that the transactions are implemented correctly. The principle of separation of duty requires that the certifier and the implementors be different people. In order for the transaction to corrupt the data (either by illicitly changing the data or by leaving the data in an inconsistent state), two different people must either make similar mistakes or collude to certify the well-formed transaction as correct.

<li><a href="#">Authentication

<li><a href="#">Subjects act on behalf of some other, external entity. The identity of that entity controls the actions that its associated subjects may take. Hence, the subjects must bind to the identity of that external entity. Authentication is the binding of an identity to a subject. The external entity must provide information to enable the system to confirm its identity. This information comes from one (or more) of the following. 1. What the entity knows (such as passwords or secret information) 2. What the entity has (such as a badge or card) 3. What the entity is (such as fingerprints or retinal characteristics) 4. Where the entity is (such as in front of a particular terminal).

<li><a href="#">The authentication process consists of obtaining the authentication information from an entity, analyzing the data, and determining if it is associated with that entity. This means that the computer must store some information about the entity.

<li><a href="#">A password is information associated with an entity that confirms the entity's identity. Passwords are an example of an authentication mechanism based on what people know: the user supplies a password, and the computer validates it. If the password is the one associated with the user, that user's identity is authenticated. If not, the password is rejected and the authentication fails.

<li><a href="#">The simplest attack against a password-based system is to guess passwords. A dictionary attack is the guessing of a password by repeated trial and error. The name of this attack comes from the list of words (a dictionary) used for guesses. The dictionary may be a set of strings in random order or (more usually) a set of strings in decreasing order of probability of selection.

<li><a href="#">Psychological studies have shown that humans can repeat with perfect accuracy about eight meaningful items, such as digits, letters, or words. If random passwords are eight characters long, humans can remember one such password. So a person who is assigned two random passwords must write them down.

<li><a href="#">A compromise between using random, unmemorizable passwords and writing passwords down is to use pronounceable passwords. The advantage of pronounceable passwords is that fewer phonemes need to be used to reach some limit, so that the user must memorize chunks of characters rather than the individual characters themselves.

<li><a href="#">Rather than selecting passwords for users, one can constrain what passwords users are allowed to select. This technique, called proactive password selection, enables users to propose passwords they can remember, but rejects any that are deemed too easy to guess.

<li><a href="#">Some categories of passwords that researchers have found easy to guess are as follows: Passwords based on account names. Dictionary words. Patterns from the keyboard. Passwords shorter than six characters. Passwords containing only digits. Passwords used in the past. Concatenations of dictionary words. Dictionary words preceded or followed by digits, punctuation marks, or spaces. Dictionary words with all vowels deleted. Passwords with too many characters in common with the previous (current) password.

<li><a href="#">Good passwords can be constructed in several ways. A password containing at least one digit, one letter, one punctuation symbol, and one control character is usually quite strong. A second technique is to pick a verse from an obscure poem (or an obscure verse from a well known poem) and pick the characters for the string from its letters.

<li><a href="#">Passwords have the fundamental problem that they are reusable. If an attacker sees a password, she can later replay the password. The system cannot distinguish between the attacker and the legitimate user, and allows access. An alternative is to authenticate in such a way that the transmitted password changes each time. Then, if an attacker replays a previously used password, the system will reject it.

<li><a href="#">A one-time password is a password that is invalidated as soon as it is used. The problems in any one-time password scheme are the generation of random passwords and the synchronization of the user and the system. The former problem is solved by using a cryptographic hash function or enciphering function such as the DES, and the latter by having the system inform the user which password it expects"for example, by having all the user's passwords numbered and the system providing the number of the one-time password it expects.

<li><a href="#">Design Principles

<li><a href="#">Saltzer and Schroeder describe eight principles for the design and implementation of security mechanisms. The principles draw on the ideas of simplicity and restriction. Simplicity makes designs and mechanisms easy to understand. More importantly, less can go wrong with simple designs. Restriction: minimizing the interaction of system components minimizes the number of sanity checks on data being transmitted from one component to another. Simplicity also reduces the potential for inconsistencies within a policy or set of policies. Restriction minimizes the power of an entity. The entity can access only information it needs. Entities can communicate with other entities only when necessary, and in as few (and narrow) ways as possible.

<li><a href="#">The principle of least privilege states that a subject should be given only those privileges that it needs in order to complete its task. If a subject does not need an access right, the subject should not have that right. Furthermore, the function of the subject (as opposed to its identity) should control the assignment of rights. If a specific action requires that a subject's access rights be augmented, those extra rights should be relinquished immediately on completion of the action. In practice, most systems do not have the granularity of privileges and permissions required to apply this principle precisely. The designers of security mechanisms then apply this principle as best they can.

<li><a href="#">The principle of fail-safe defaults states that, unless a subject is given explicit access to an object, it should be denied access to that object. This principle requires that the default access to an object is none. Whenever access, privileges, or some security-related attribute is not explicitly granted, it should be denied. Moreover, if the subject is unable to complete its action or task, it should undo those changes it made in the security state of the system before it terminates. This way, even if the program fails, the system is still safe.

<li><a href="#">The principle of economy of mechanism states that security mechanisms should be as simple as possible. If a design and implementation are simple, fewer possibilities exist for errors. The checking and testing process is less complex because fewer components and cases need to be tested. Complex mechanisms often make assumptions about the system and environment in which they run. If these assumptions are incorrect, security problems may result.

<li><a href="#">The principle of complete mediation requires that all accesses to objects be checked to ensure that they are allowed. Whenever a subject attempts to read an object, the operating system should mediate the action. First, it determines if the subject is allowed to read the object. If so, it provides the resources for the read to occur. If the subject tries to read the object again, the system should check that the subject is still allowed to read the object.

<li><a href="#">The principle of open design states that the designers and implementers of a program must not depend on secrecy of the details of their design and implementation to ensure security. Others can ferret out such details either through technical means, such as disassembly and analysis, or through nontechnical means, such as searching through garbage receptacles for source code listings (called dumpster-diving).

<li><a href="#">If the strength of the program's security depends on the ignorance of the user, a knowledgeable user can defeat that security mechanism. The term security through obscurity captures this concept exactly. Experience has shown that such secrecy adds little if anything to the security of the system. Worse, it gives an aura of strength that is all too often lacking in the actual implementation of the system.

<li><a href="#">The principle of separation of privilege states that a system should not grant permission based on a single condition. Company checks for more than $75,000 must be signed by two officers of the company. If either does not sign, the check is not valid. The two conditions are the signatures of both officers. Similarly, systems and programs granting access to resources should do so only when more than one condition is met. This provides a fine-grained control over the resource as well as additional assurance that the access is authorized.

<li><a href="#">The principle of least common mechanism states that mechanisms used to access resources should not be shared. Sharing resources provides a channel along which information can be transmitted, and so such sharing should be minimized. In practice, if the operating system provides support for virtual machines, the operating system will enforce this privilege automatically to some degree. Otherwise, it will provide some support (such as a virtual memory space) but not complete support (because the file system will appear as shared among several processes).

<li><a href="#">The principle of psychological acceptability states that security mechanisms should not make the resource more difficult to access than if the security mechanisms were not present. Configuring and executing a program should be as easy and as intuitive as possible, and any output should be clear, direct, and useful. If security-related software is too complicated to configure, system administrators may unintentionally set up the software in a nonsecure manner.

<li><a href="#">Assurance

<li><a href="#">When looked on as an absolute, creating a secure system is an ultimate, albeit unachievable, goal. As soon as we have figured out how to address one type of attack on a system, other types of attacks occur. In reality, we cannot yet build systems that are guaranteed to be secure or to remain secure over time.

<li><a href="#">Intuitively, trust is a belief or desire that a computer entity will do what it should to protect resources and be safe from attack. However, in the realm of computer security, trust has a very specific meaning. An entity is trustworthy if there is sufficient credible evidence leading one to believe that the system will meet a set of given requirements. Trust is a measure of trustworthiness, relying on the evidence provided.

<li><a href="#">Security assurance , or simply assurance, is confidence that an entity meets its security requirements, based on specific evidence provided by the application of assurance techniques. Examples of assurance techniques include the use of a development methodology, formal methods for design analysis, and testing. Evidence specific to a particular technique may be simplistic or may be complex and fine-grained.

<li><a href="#">A trusted system is a system that has been shown to meet well-defined requirements under an evaluation by a credible body of experts who are certified to assign trust ratings to evaluated products and systems. These methodologies provide increasing levels of trust, each level having more stringent assurance requirements than the previous one. When experts evaluate and review the evidence of assurance, they provide a check that the evidence amassed by the vendor is credible to disinterested parties and that the evidence supports the claims of the security requirements.

<li><a href="#">Accidental or unintentional failures of computer systems, as well as intentional compromises of security mechanisms, can lead to security failures. Neumann describes nine types of problem sources in computer systems: 1. Requirements definitions, omissions, and mistakes 2. System design flaws 3. Hardware implementation flaws, such as wiring and chip flaws 4. Software implementation errors, program bugs, and compiler bugs 5. System use and operation errors and inadvertent mistakes 6. Willful system misuse 7. Hardware, communication, or other equipment malfunction 8. Environmental problems, natural causes, and acts of God 9. Evolution, maintenance, faulty upgrades, and decommissions.

<li><a href="#">Although security policies define security for a particular system, the policies themselves are created to meet needs. These needs are the requirements. A requirement is a statement of goals that must be satisfied. A statement of goals can vary from generic, high-level goals to concrete, detailed design considerations. The term security objectives refers to the high-level security issues and business goals, and the term security requirements refers to the specific and concrete issues. Selecting the right security requirements for a computer entity requires an understanding of the intended use of that entity as well as of the environment in which it must function.

<li><a href="#">The goal of assurance is to show that an implemented and operational system meets its security requirements throughout its life cycle. Because of the difference in the levels of abstraction between high-level security requirements and low-level implementation details, the demonstration is usually done in stages. Different assurance techniques apply to different stages of system development. For this reason, it is convenient to classify assurance into policy assurance, design assurance, implementation assurance, and operational or administrative assurance.

<li><a href="#">Policy assurance is the evidence establishing that the set of security requirements in the policy is complete, consistent, and technically sound. Once the proper requirements have been defined, justified, and approved for the system, the design and development process can begin with confidence.

<li><a href="#">Design assurance is the evidence establishing that a design is sufficient to meet the requirements of the security policy. Implementation assurance is the evidence establishing that the implementation is consistent with the security requirements of the security policy. In practice, implementation assurance shows that the implementation is consistent with the design, which design assurance showed was consistent with the security requirements found in the security policy.

<li><a href="#">Operational or administrative assurance is the evidence establishing that the system sustains the security policy requirements during installation, configuration, and day-to-day operation. One fundamental operational assurance technique is a thorough review of product or system documentation and procedures, to ensure that the system cannot accidentally be placed into a nonsecure state. This emphasizes the importance of proper and complete documentation for computer applications, systems, and other entities.

<li><a href="#">Like performance, security is an integral part of a computer system. It should be integrated into the system from the beginning, rather than added on later. Inclusion of many features often leads to complexity, which limits the ability to analyze the system, which in turn lowers the potential level of assurance. Systems in which security mechanisms are added to a previous product are not as amenable to extensive analysis as those that are specifically built for security.

<li><a href="#">Penetration Testing

<li><a href="#">A computer system is more than hardware and software; it includes the policies, procedures, and organization under which that hardware and software is used. Lapses in security can arise from any of these areas or from any combination of these areas. Thus, it makes little sense to restrict the study of vulnerabilities to hardware and software problems.

<li><a href="#">When someone breaks into a computer system, that person takes advantage of lapses in procedures, technology, or management (or some combination of these factors), allowing unauthorized access or actions. The specific failure of the controls is called a vulnerability or security flaw; using that failure to violate the site security policy is called exploiting the vulnerability. One who attempts to exploit the vulnerability is called an attacker.

<li><a href="#">Penetration testing is a testing technique, not a proof technique. It can never prove the absence of security flaws; it can only prove their presence. To be meaningful, a formal verification proof must include all external factors. Incorrect configuration, maintenance, or operation of the program or system may introduce flaws that formal verification will not detect.

<li><a href="#">A penetration study is a test for evaluating the strengths of all security controls on the computer system. The goal of the study is to violate the site security policy. A penetration study (also called a tiger team attack or red team attack) is not a replacement for careful design and implementation with structured testing. Unlike other testing and verification technologies, it examines procedural and operational controls as well as technological controls.

<li><a href="#">A penetration test is an authorized attempt to violate specific constraints stated in the form of a security or integrity policy. This formulation implies a metric for determining whether the study has succeeded. Should goals be nebulous, interpretation of the results will also be nebulous, and the test will be less useful than if the goals were stated precisely. Example goals of penetration studies are gaining of read or write access to specific objects, files, or accounts; gaining of specific privileges; and disruption or denial of the availability of objects.

<li><a href="#">A penetration test is designed to characterize the effectiveness of security mechanisms and controls to attackers. To this end, these studies are conducted from an attacker's point of view, and the environment in which the tests are conducted is that in which a putative attacker would function. Different attackers, however, have different environments; for example, insiders have access to the system, whereas outsiders need to acquire that access. This suggests a layering model for a penetration study.

<li><a href="#">External attacker with no knowledge of the system . This layer is normally skipped in penetration testing because it tells little about the security of the system itself.



<li><a href="#">External attacker with access to the system . At this level, the testers have access to the system and can proceed to log in or to invoke network services available to all hosts on the network (such as electronic mail). They must then launch their attack. Common forms of attack at this stage are guessing passwords, looking for unprotected accounts, and attacking network servers. Implementation flaws in servers often provide the desired access.

<li><a href="#">Internal attacker with access to the system . At this level, the testers have an account on the system and can act as authorized users of the system. The test typically involves gaining unauthorized privileges or information and, from that, reaching the goal. At this stage, the testers acquire (or have) a good knowledge of the target system, its design, and its operation.

<li><a href="#">Vulnerability Analysis

<li><a href="#">The usefulness of a penetration study comes from the documentation and conclusions drawn from the study and not from the success or failure of the attempted penetration. The Flaw Hypothesis Methodology was developed at System Development Corporation and provides a framework for penetration studies. It consists of five steps.

<li><a href="#">Information gathering . In this step, the testers become familiar with the system's functioning. They examine the system's design, its implementation, its operating procedures, and its use. The testers become as familiar with the system as possible.

<li><a href="#">Flaw hypothesis . Drawing on the knowledge gained in the first step, and on knowledge of vulnerabilities in other systems, the testers hypothesize flaws of the system under study.

<li><a href="#">Flaw testing . The testers test their hypothesized flaws. If a flaw does not exist (or cannot be exploited), the testers go back to step 2. If the flaw is exploited, they proceed to the next step.

<li><a href="#">Flaw generalization . Once a flaw has been successfully exploited, the testers attempt to generalize the vulnerability and find others similar to it. They feed their new understanding (or new hypothesis) back into step 2 and iterate until the test is concluded.

<li><a href="#">Flaw elimination . The testers suggest ways to eliminate the flaw or to use procedural controls to ameliorate it.

<li><a href="#">Information Gathering and Flaw Hypothesis : The testers devise a model of the system, or of its components, and then explore each aspect of the designs for internal consistency, incorrect assumptions, and potential flaws. If the testers have access to design documents and manuals, they can often find parts of the specification that are imprecise or incomplete. If a privileged user (such as root on UNIX systems or administrator on Windows systems) is present, the way the system manages that user may reveal flaws. Wherever the manuals suggest a limit or restriction, the testers try to violate it; wherever the manuals describe a sequence of steps to perform an action involving privileged data or programs, the testers omit some steps. More often than not, this strategy will reveal security flaws.

<li><a href="#">Flaw Testing : Once the testers have hypothesized a set of flaws, they determine the order in which to test the flaws. The priority is a function of the goals of the test. Assigning priorities is a matter of informed judgment, which emphasizes the need for testers to be familiar with the environment and the system. When a system must be tested, it should be backed up and all users should be removed from it. This precautionary measure saves grief should the testing go awry. The notes of the test must be complete enough to enable another tester to duplicate the test or the exploitation on request; thus, precise notes are essential.

<li><a href="#">Flaw Generalization and Flaw Elimination : As testers successfully penetrate the system (either through analysis or through analysis followed by testing), classes of flaws begin to emerge. The testers must confer enough to make each other aware of the nature of the flaws. The flaw elimination step is often omitted because correction of flaws is not part of the penetration. However, the flaws uncovered by the test must be corrected. For example, the TCSEC requires that any flaws uncovered by penetration testing be corrected.

<li><a href="#">Penetration testing is no substitute for good, thorough specification, rigorous design, careful and correct implementation, and meticulous testing. It is, however, a very valuable component of the final stage, testing. Properly done, penetration tests examine the design and implementation of security mechanisms from the point of view of an attacker. The knowledge and understanding gleaned from such a viewpoint is invaluable.

<li><a href="#">In 1992, Landwehr, Bull, McDermott, and Choi developed a taxonomy to help designers and operators of systems enforce security. They tried to answer three questions: how did the flaw enter the system, when did it enter the system, and where in the system is it manifest? They built three different classification systems, one to answer each of the three questions, and classified more than 50 vulnerabilities in these schemes.

<li><a href="#">The first classification scheme classified vulnerabilities by genesis. The investigators felt that because most security flaws were inadvertent, better design and coding reviews could eliminate many of them; but if the flaws were intentional, measures such as hiring more trustworthy designers and programmers and doing more security-related testing would be more appropriate.

<li><a href="#">The second scheme classified vulnerabilities by time of introduction. The investigators wanted to know if security errors were more likely to be introduced at any particular point in the software life cycle in order to determine if focusing efforts on security at any specific point would be helpful.

<li><a href="#">The third scheme classified vulnerabilities by location of the flaw. The intent is to capture where the flaw manifests itself and to determine if any one location is more likely to be flawed than any other. If so, focusing resources on that location would improve security.

<li><a href="#">Landwehr's et al taxonomy differs from the others in that it focuses on social processes as well as technical details of flaws. In order to classify a security flaw correctly on the time of introduction and genesis axes, either the precise history of the particular flaw must be known or the classifier must make assumptions. This ambiguity is unsettling because this information is not always available. However, when available, this information is quite useful, and the study was the first to approach the problem of reducing vulnerabilities by studying the environments in which they were introduced.

<li><a href="#">Intrusion Detection

<li><a href="#">Computer systems that are not under attack exhibit several characteristics. Denning hypothesized that systems under attack fail to meet at least one of these characteristics.

<li><a href="#">&#183; The actions of users and processes generally conform to a statistically predictable pattern. A user who does only word processing when using the computer is unlikely to perform a system maintenance function.

<li><a href="#">&#183; The actions of users and processes do not include sequences of commands to subvert the security policy of the system. In theory, any such sequence is excluded; in practice, only sequences known to subvert the system can be detected.

<li><a href="#">&#183; The actions of processes conform to a set of specifications describing actions that the processes are allowed to do (or not allowed to do).

<li><a href="#">The characteristics listed above guide the detection of intrusions. Once the province of the technologically sophisticated, attacks against systems have been automated. So a sophisticated attack need not be the work of a sophisticated attacker. An attack tool is an automated script designed to violate a security policy.

<li><a href="#">Denning suggests automation of the intrusion detection process. Her specific hypothesis is that exploiting vulnerabilities requires an abnormal use of normal commands or instructions, so security violations can be detected by looking for abnormalities. Her model is very general and includes abnormalities such as deviation from usual actions ( anomaly detection), execution of actions that lead to break-ins (misuse detection), and actions inconsistent with the specifications of privileged programs ( specification-based detection).

<li><a href="#">Systems that do this are called intrusion detection systems (IDS). Their goals are fourfold: (1) Detect a wide variety of intrusions. (2) Detect intrusions in a timely fashion. (3) Present the analysis in a simple, easy-to-understand format. (4) Be accurate.

<li><a href="#">Anomaly models use a statistical characterization, and actions or states that are statistically unusual are classified as bad. Misuse models compare actions or states with sequences known to indicate intrusions, or sequences believed to indicate intrusions, and classify those sequences as bad. Specification-based models classify states that violate the specifications as bad. In practice, models are often combined, and intrusion detection systems use a mixture of two or three different types of models.

<li><a href="#">Anomaly detection uses the assumption that unexpected behavior is evidence of an intrusion. Implicit is the belief that some set of metrics can characterize the expected behavior of a user or a process. Anomaly detection analyzes a set of characteristics of the system and compares their behavior with a set of expected values. It reports when the computed statistics do not match the expected measurements.

<li><a href="#">Misuse detection determines whether a sequence of instructions being executed is known to violate the site security policy being executed. If so, it reports a potential intrusion. Modeling of misuse requires a knowledge of system vulnerabilities or potential vulnerabilities that attackers attempt to exploit. The intrusion detection system incorporates this knowledge into a rule set. When data is passed to the intrusion detection system, it applies the rule set to the data to determine if any sequences of data match any of the rules. If so, it reports that a possible intrusion is underway.

<li><a href="#">Specification detection looks for states known not to be good, and when the system enters such a state, it reports a possible intrusion. Specification-based detection determines whether or not a sequence of instructions violates a specification of how a program, or system, should execute. If so, it reports a potential intrusion. Specification-based intrusion detection is in its infancy. Among its appealing qualities are the formalization (at a relatively low level) of what should happen. This means that intrusions using unknown attacks will be detected.

<li><a href="#">Intrusion Response

<li><a href="#">Once an intrusion is detected, how can the system be protected? The field of intrusion response deals with this problem. Its goal is to handle the (attempted) attack in such a way that damage is minimized (as determined by the security policy). Some intrusion detection mechanisms may be augmented to thwart intrusions. Otherwise, the security officers must respond to the attack and attempt to repair any damage.

<li><a href="#">In the context of response, prevention requires that the attack be identified before it completes. The defenders then take measures to prevent the attack from completing. This may be done manually or automatically.

<li><a href="#">Jailing of attackers is an approach that allows the attackers to think that their attacks have succeeded but places them in a confined area in which their behavior can be controlled and, if necessary, manipulated.

<li><a href="#">Intrusion handling consists of six phases.

<li><a href="#">&#183; Preparation for an attack. This step occurs before any attacks are detected. It establishes procedures and mechanisms for detecting and responding to attacks.

<li><a href="#">&#183; Identification of an attack. This triggers the remaining phases.

<li><a href="#">&#183; Containment of the attack. This step limits the damage as much as possible.

<li><a href="#">&#183; Eradication of the attack. This step stops the attack and blocks further similar attacks.

<li><a href="#">&#183; Recovery from the attack. This step restores the system to a secure state (with respect to the site security policy).

<li><a href="#">&#183; Follow-up to the attack. This step involves taking action against the attacker, identifying problems in the handling of the incident, and recording lessons learned (or lessons not learned that should be learned).

<li><a href="#">Honeypots , sometimes called decoy servers, are servers that offer many targets for attackers. The targets are designed to entice attackers to take actions that indicate their goals. Honeypots are also instrumented and closely monitored. When a system detects an attack, it takes actions to shift the attacker onto a honeypot system. The defenders can then analyze the attack without disrupting legitimate work or systems.

<li><a href="#">Firewalls are systems that sit between an organization's internal network and some other external network (such as the Internet). The firewall controls access from the external network to the internal network and vice versa. The advantage of firewalls is that they can filter network traffic before it reaches the target host. They can also redirect network connections as appropriate, or throttle traffic to limit the amount of traffic that flows into (or out of) the internal network.

<li><a href="#">An organization may have several firewalls on its perimeter, or several organizations may wish to coordinate their responses. The Intruder Detection and Isolation Protocol provides a protocol for coordinated responses to attacks. When a connection passes through a member of an IDIP domain, the system monitors the connection for intrusion attempts. If one occurs, the system reports the attempt to its neighbors. Kahn and Zurko suggest that IDIP, or a similar protocol, should be widely deployed throughout the Internet to handle flooding attacks. They argue that economic and other incentives will encourage Internet Service Providers and other network providers to cooperate in suppressing distributed flooding attacks.

<li><a href="#">Counterattacking , or attacking the attacker, takes two forms. The first form involves legal mechanisms, such as filing criminal complaints. This requires protecting a chain of evidence so that legal authorities can establish that the attack was real (in other words, that the attacked site did not invent evidence) and that the evidence can be used in court.

<li><a href="#">The second form is a technical attack, in which the goal is to damage the attacker seriously enough to stop the current attack and discourage future attacks. This approach has several important consequences that must be considered.

<li><a href="#">The counterattack may harm an innocent party. The attacker may be impersonating another site. In this case, the counterattack could damage a completely innocent party, putting the counterattackers in the same position as the original attackers. Alternately, the attackers may have broken into the site from which the attack was launched. Attacking that host does not solve the problem. It merely eliminates one base from which future attacks might be launched.

<li><a href="#">The counterattack may have side effects. For example, if the counterattack consists of flooding a specific target, the flood could block portions of the network that other parties need to transit, which would damage them.

<li><a href="#">The counterattack is antithetical to the shared use of a network. Networks exist to share data and resources and provide communication paths. By attacking, regardless of the reason, the attackers make networks less usable because they absorb resources and make threats more immediate.

<li><a href="#">The counterattack may be legally actionable. If an attacker can be prosecuted or sued, it seems reasonable to assume that one who responds to the attack by counterattacking can also be prosecuted or sued, especially if other innocent parties are damaged by the counterattack.

<li><a href="#">Data Classification

<li><a href="#">Classification of data should reflect the principle of least privilege. Data should be separated in such a way that the ability to view one class of data does not imply the ability to view another class of data. Also, the policy and all its rules are not secret, reflecting the principle of open design. 5 classes of data:

<li><a href="#">&#183; Public data (PD) is available to anyone. It includes product specifications, price information, marketing literature, and any other data that will help a company sell without compromising its secrets.

<li><a href="#">&#183; Development data for existing products (DDEP) is available only internally. Because of pending lawsuits, it must be available to the company lawyers and officers as well as to the developers. It is kept secret from all others.

<li><a href="#">&#183; Development data for future products (DDFP) is available to the developers only.

<li><a href="#">&#183; Corporate data (CpD) includes legal information that is privileged and information about corporate actions that is not to become known publicly (such as actions that may affect stock values). The corporate officials and lawyers need access to this information; no one else does.

<li><a href="#">&#183; Customer data (CuD) is data that customers supply, such as credit card information. The company protects this data as strongly as it protects its own data.

<li><a href="#">The user classes are based on the same principles as the classes of data: separation of privilege and least privilege. Some users may be placed in multiple classes. If so, an underlying assumption of the model is that they will not bypass the restrictions by copying data from one class to another without using the mechanisms provided for that purpose. Four classes of people may access data.

<li><a href="#">&#183; Outsiders (members of the public) get access to some of the company's data such as prices, product descriptions, and public corporate information.

<li><a href="#">&#183; Developers get access to both classes of development data.

<li><a href="#">&#183; Corporation executives (corporation counsel, members of the board of directors, and other executives) get access to corporate data.

<li><a href="#">&#183; Employees get access to customer data only.



<li><a href="#">Network Organization

<li><a href="#">Networks should be partitioned into several parts, with guards between parts to prevent information from leaking. Each type of data resides in one of the parts. This is a fairly standard corporate network, with one part available to the public and a second part available only internally. The DMZ (Demilitarized Zone) is a portion of a network that separates a purely internal network from an external network.

<li><a href="#">When information moves from the Internet to the internal network, confidentiality is not at issue. However, integrity is. The guards between the Internet and the DMZ, and between the DMZ and the internal network, must not accept messages that will cause servers to work incorrectly or to crash. When information moves from the internal network to the Internet, confidentiality and integrity are both at issue.

<li><a href="#">A firewall is a host that mediates access to a network, allowing and disallowing certain types of access on the basis of a configured security policy. This firewall accepts or rejects messages on the basis of external information, such as destination addresses or ports, rather than on the basis of the contents of the message.

<li><a href="#">A filtering firewall performs access control on the basis of attributes of the packet headers, such as destination addresses, source addresses, and options. Access control lists provide a natural mechanism for representing these policies. This contrasts with the second type of firewall, which never allows such a direct connection. Instead, special agents called proxies control the flow of information through the firewall.

<li><a href="#">A proxy is an intermediate agent or server that acts on behalf of an endpoint without allowing a direct connection between the two endpoints. A proxy (or applications level) firewall uses proxies to perform access control. A proxy firewall can base access control on the contents of packets and messages, as well as on attributes of the packet headers.

<li><a href="#">The public cannot communicate directly with any system in the internal network, nor can any system in the internal network communicate directly with other systems on the Internet (beyond the outer firewall). The systems in the DMZ serve as mediators, with the firewalls providing the guards. The goals of the outer firewall are to restrict public access to the company's corporate network and to restrict the company's access to the Internet. This arises from the duality of information flow.

<li><a href="#">Four servers reside in the DMZ. They are the mail, WWW, DNS, and log servers.

<li><a href="#">The mail server in the DMZ performs address and content checking on all electronic mail messages. The goal is to hide internal information from the outside while being transparent to the inside. The Web server accepts and services requests from the Internet. It does not contact any servers or information sources within the internal network. This means that if the Web server is compromised, the compromise cannot affect internal hosts. The DMZ DNS server contains directory name service information about those hosts that the DMZ servers must know. The limited information in the DNS server reflects the principle of least privilege because those entries are sufficient for the systems in the DMZ. The log server performs an administrative function. All DMZ machines have logging turned on. In the event of a compromise (or an attempted compromise), these logs will be invaluable in assessing the method of attack, the damage (or potential damage), and the best response.

<li><a href="#">Ideally, the operating systems of the server computers should be very small kernels that provide only the system support services necessary to run the appropriate servers. In practice, the operating systems are trusted operating systems (developed using assurance techniques, or"more commonly"commercial operating systems in which all unnecessary features and services have been disabled. This minimizes the operations that a server can perform on behalf of a remote process. Hence, even if the server is compromised, the attacker cannot use it to compromise other hosts such as the inner firewall.

<li><a href="#">User Security

<li><a href="#">The components of users' policies that we focus on are as follows.

<li><a href="#">U1. Only users have access to their accounts.

<li><a href="#">U2. No other user can read or change a file without the owner's permission.

<li><a href="#">U3. Users shall protect the integrity, confidentiality, and availability of their files.

<li><a href="#">U4. Users shall be aware of all commands that they enter, or that are entered on their behalf.

<li><a href="#">Component U1 requires that users protect access to their accounts. Consider the ways in which users gain access to their accounts. These points of entry are ideal places for attackers to attempt to masquerade as users. Hence, they form the first locus of users' defenses.

<li><a href="#">Ideally, passwords should be chosen randomly. In practice, such passwords are difficult to remember. So, either passwords are not assigned randomly, or they require that some information be written down. Writing down passwords is popularly considered to be dangerous. In reality, the degree of danger depends on the environment in which the system is accessed and on the manner in which the password is recorded.

<li><a href="#">The first potential access authentication attack arises from the lack of mutual authentication on most systems. An attacker may place a program at the access point that emulates the login prompt sequence. Then, if the user has a reusable password, the name and password are captured.

<li><a href="#">The second potential attack arises from an attacker reading the password as it is entered. At a later date, the attacker can reuse the password. This differs from the first attack in that it succeeds even when the user and system mutually authenticate each other.

<li><a href="#">As part of the login procedure, many systems print useful information. If the date, time, and location of the last successful login are shown, the user can verify that no one has used her account since she last did. If the access point is shown, the user can determine if some program is intercepting and rerouting her communications. Policy component U1 suggests that the user should be alert when logging in. If something suspicious occurs, or the link to the system is not physically or cryptographically protected, an unauthorized user may acquire access to the system.

<li><a href="#">The notion of trusted hosts comes from the belief that if two hosts are under the same administrative control, each can rely on the other to authenticate a user. It allows certain mechanisms, such as backups, to be automated without placing passwords or cryptographic keys on the system. The trusted host mechanism requires accurate identification of the connecting host. The primary identification token of a host is its IP address, but the authentication mechanism can be either the IP address itself or a challenge-response exchange based on cryptography. Using the latter prevents IP spoofing.

<li><a href="#">Users must protect confidentiality and integrity of the files to satisfy policy component U2. To this end, they use the protection capabilities of the system to constrain access. Complicating the situation is the interpretation of permissions on the containing directories. Many systems allow users to specify a template of permissions to be given to a file when it is created. The owner can then modify this set as required.

<li><a href="#">Group access provides a selected set of users with the same access rights. The problem is that the membership of the group is not under the control of the owner of the file. This has an advantage and a disadvantage. The advantage arises when the group is used as a role. Then, as users are allowed to assume the role, their access to the file is altered. Because the owner of the file is concerned only with controlling access of those role users, reconfiguration of the access to the role reconfigures user access to the file, which is what the user wants.

<li><a href="#">The disadvantage arises when a group is used as a shorthand for a set of specific users. If the membership of the group changes, unauthorized users may obtain access to the file, or authorized users may be denied access to the file. In general, users should limit access as much as possible when creating new files. So ACLs and C-Lists should include as few entries as possible, and permissions for each entry should be as restrictive as possible.

<li><a href="#">Users communicate with the system through devices. The devices may be virtual, such as network ports, or physical, such as terminals. Policy components U1 and U4 require that these devices be protected so that the user can control what commands are sent to the system in her name and so that others are prevented from seeing her interactions. Devices that allow any user to write to them can pose serious security problems. Unless necessary for the correct functioning of the system, devices should restrict write access as much as possible.

<li><a href="#">The basis for encryption is trust. Anyone who can alter the programs used to encipher and decipher the files, or any of the supporting tools (such as the operating system), can also obtain the cryptographic keys or the cleartext data itself. For this reason, unless users trust the privileged users, and trust that other users cannot acquire the privileges needed to read memory, swap space, or alter the relevant programs, the sensitive data should never be on the system in cleartext.


</body>
</ul>
</div>
<script>
function myFunction() {
    var input, filter, ul, li, a, i, txtValue;
    input = document.getElementById("myInput");
    filter = input.value.toUpperCase();
    ul = document.getElementById("myUL");
    li = ul.getElementsByTagName("li");
    for (i = 0; i < li.length; i++) {
        a = li[i].getElementsByTagName("a")[0];
        txtValue = a.textContent || a.innerText;
        if (txtValue.toUpperCase().indexOf(filter) > -1) {
            li[i].style.display = "";
        } else {
            li[i].style.display = "none";
        }
    }
}
</script>
<script src="hilitor.js"></script>
<script>
var myHilitor = new Hilitor("content"); // id of the element to parse
// myHilitor.setBreakRegExp(new RegExp('[^\w -]+', "g")); // expanded to include spaces
myHilitor.apply();
</script>
<script>
  window.addEventListener("DOMContentLoaded", function(e) {
    var myHilitor2 = new Hilitor("playground");
    myHilitor2.setMatchType("left");
    document.getElementById("keywords").addEventListener("keyup", function(e) {
      myHilitor2.apply(this.value);
    }, false);
  }, false);
</script>

</html>
