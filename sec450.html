<html>
<title>SANS Notes</title>
<meta name="robots" content="noindex,nofollow" />

<link rel="stylesheet" href="w2.css">
<link rel="stylesheet" href="w3-theme-black.css">
<link rel="stylesheet" href="roboto.css">
<link rel="stylesheet" href="font-awesome.min.css">
<head>
<style>



html, body {
-webkit-user-select: none;
-moz-user-select: none;
-ms-user-select: none;
user-select: none;
oncontextmenu="return false";
onselectstart="return false";
ondragstart="return false";
background-color: #f0f0f0;
}



* {
  box-sizing: border-box;
}

.w3-sidebar {
	z-index: 3;
	width: 345px;
	top: 30px;
	bottom: 0;
	height: inherit;
	text-align: justify
	height: 100%;
	background-color: #f0f0f0;
	position: fixed;
	overflow: auto;
	line-height: 0.2
}
.w3-searchbox {
	height: 25%;
	width: 110%;
	left:405px;
	top: 23px;
	overflow: hidden;
	background-color: black;
	position: fixed;
	z-index: 1;
	line-height: 2;
 margin-top: -7px; 

}

.w3-topbar {
	width: 100%;
	height:5%;
	background-color: black;
	position: fixed;
	top: -3px;
	overflow: hidden;

	
}


#myInput {
  background-repeat: no-repeat;
  width: 40%;
  border: 1px solid #ddd;
 margin-top: 20px; 
 margin-bottom: 12px;
  margin-left: 95px;
  text-align: center;
 font-family: "Roboto", sans-serif

}

#keywords {
  background-repeat: no-repeat;
  width: 20%;
  border: 1px solid #ddd;
  border: 1px solid #ddd;
  margin-bottom: 12px;
  margin-left: 120px;

 text-align: center;
 font-family: "Roboto", sans-serif

}
#myUL {
  list-style-type: none;
  padding: 0;
  text-align: justify;
  margin-top: 150px;
  width: 70%;
}

#myUL li a {
  margin-top: -1px; 
  padding: 8px;
  text-decoration: none;
  font-size: 14px;
  color: black;
  display: block
}

}
</style>
</head>

<body>
<!-- Topbar -->
<div class="w3-topbar"></div>

<!-- Sidebar -->
<nav class="w3-sidebar w3-bar-block w3-collapse w3-small w3-theme-l5" id="mySidebar">

<BR>
<p>
<BR>
<center><H3>SANS COURSE NOTES</H3></center>
<BR>
<p>
<BR>
<a class=" w3-button w3-hover-blue" href="sec401.html">SEC 401 - Security Essentials</a>
<a class="w3-button w3-hover-blue" href="sec450.html">SEC 450 - Blue Team Fundamentals</a>
<a class="w3-button w3-hover-green" href="for500.html">FOR 500 - Windows Forensic Analysis</a>
<a class="w3-button w3-hover-blue" href="sec502.html">SEC 502 - Perimeter Protection</a>
<a class="w3-button w3-hover-red" href="sec504.html">SEC 504 - Hacker Tools</a>
<a class="w3-button w3-hover-blue" href="sec506.html">SEC 506 - Linux/Unix Security</a>
<a class="w3-button w3-hover-green" href="for508.html">FOR 508 - Incident Response Forensics</a>
<a class="w3-button w3-hover-blue" href="sec511.html">SEC 511 - Continuous Monitoring</a>
<a class="w3-button w3-hover-blue" href="sec555.html">SEC 555 - SIEM with Tactical Analytics</a>
<P>
<center>
<a class="w3-button" href="standards.html">Security Standards</a><br>
<a class="w3-button" href="books.html">Security Books</a><br>
<a class="w3-button" href="tools.html">Security Tools</a>
</center>
</nav>
<nav class="w3-searchbox" id="searchbox">

<input type="text" id="myInput" onkeyup="myFunction()" placeholder="Filter Paragraph by Keyword" title="Search">
<form method="GET" onsubmit="myHilitor.apply(hilite.value); return false;">
<input type="text" id="keywords" size="20" name="hilite" placeholder="Highlight Multiple Keywords">
<input type="submit" value="Apply">
<input type="button" value="Remove" onclick="myHilitor.remove();">
</span>
</form>

</nav>


<div class="w3-main w3-theme-l5" style="margin-left:300px"> 
<div class="w3-row w3-padding-64">               
<h2 class="w3-text-teal"></h2>
<div id="playground">


<center>




<ul id="myUL">


<p>
        Blue Team Fundamentals: Security Operations and Analysis, John Hubbard
<p>

<li><a href="#">Most attacks are financially motivated. Attackers are after anything of value that can be sold on the black market. The second most common reason is to gain a strategic advantage, otherwise known as espionage. Verizon notes that between financial gain and espionage, roughly 90% of breaches are accounted for. The most typical cybersecurity breach will consist of an external attacker looking to steal the data they can use to make a quick dollar. Knowing this can be the first step in putting up a threat-aligned defense.


<li><a href="#">The red team is an internal or external group whose job is to simulate real attacker Tactics, Techniques, and Procedures (TTPs) and challenge the Blue Team to improve their effectiveness.


<li><a href="#">A purple team is often an activity and describes an exercise where the Red and Blue Team sit together and simultaneously walk through a simulated attack. The goal is to learn real-time from each other how attacks can be launched and where their effects can be seen in the environment.


<li><a href="#">Although preventing as many attacks as possible at the perimeter is ideal, it is not realistic; therefore, we must strive toward a goal of minimizing the damage of what does manage to get through the initial layers of defense. We do this by architecting our people, process, and technology for rapid detection of a compromise in progress, and work to rapidly stop its progress before severe damage is caused.


<li><a href="#">Attack techniques and exploits will change daily, and to add to the challenge, you will always have imperfect knowledge of the situation at hand. Learning to understand the protocols in use on the network, how to read and interpret log files, how to perform high quality analysis, and how data should and shouldn't flow goes a long way toward accomplishing this mission.


<li><a href="#">At a very high level, the Security Operations Center (SOC) collects multiple types of data using various sensors and tools. Of all the data that is collected or seen, interesting items must be picked out of it at the detection stage. This process is fed by threat intelligence, attack technique knowledge, and information about your environment. Anything that is "interesting" becomes an alert and gets marked for investigation.


<li><a href="#">There will almost always be more than one item to attend to at once, so triage will be a necessary skill in this process. Once the most seemingly dangerous item is selected, we must dive in, triage, and investigate it to see if it truly is something bad going on, then, pass the case on to incident response if required.


<li><a href="#">The SOC is a function that takes both the things that have happened in the environment and what attacks look like as input, and outputs identified, minimized, and remediated incidents. As with any system, if we want better output, we need better input (this is where the phrase garbage in, garbage out comes from). For a SOC, that means either more visibility or better knowledge of what an attack looks like.


<li><a href="#">Detection : The goal of the detection function is watching the data collected from the network and endpoints and accurately identifying any potential compromises. This can be thought of as what your network and host IDS systems are doing, anti-virus, SIEM analytics, and anything else that watches everyday events and outputs alerts of possible compromise.


<li><a href="#">Triage and Investigation : The triage and analysis functions are where all the identified alerts go to get prioritized and verified. Since in nearly every SOC there will be many potentially malicious events identified, it is the primary job of the SOC analyst to sort through them for criticality and verify whether an attack has indeed occurred.


<li><a href="#">Incident Response : The incident response area is responsible for reacting to problems that are verified and ensuring the impact of the issue is minimized. In smaller SOCs, this falls under the scope of the analysts; in others, this may be a separate group called the CIRT or CSIRT. Regardless of the org structure, incident response is typically considered a core function of the blue team.


<li><a href="#">Threat Intelligence : The mission of the threat intelligence group is to collect detailed high and low-level information on attack groups interested in the organization. The goal is to help give the Blue Team a tactical and strategic advantage over the attacker. If we can anticipate attacker goals, moves, and infrastructure ahead of time, it will be much harder for adversaries to accomplish their mission.


<li><a href="#">Forensics : A specialized function focused on determining exactly what occurred during a breach. This may be traditional hard drive forensics, or something more specific such as memory analysis, malware reverse engineering, or even eDiscovery.


<li><a href="#">In order to perform the main SOC functions, there are some important pieces of information and inputs for the SOC to have on hand. One item is an overall view of how data traverses the network, and where the SOC has points of visibility. Often, in an incident, the question "would we have seen that traffic?" must be answered.


<li><a href="#">A list of critical assets is of utmost importance as well. It's hard to defend your most important data or even recognize it's in jeopardy if you don't know where it is and the systems it sits on. Once you understand what data is collected, how it is collected, and where it comes from, you will be able to intuitively know what sources you must consult to answer any given analysis question.



<li><a href="#">Policies are high-level, broad, direction-setting documents that do not go into specifics, but lay out the general requirements for configuration items. They generally answer the "what" must be done.


<li><a href="#">As opposed to Policies, standards give more specifics in that they specify "how" something gets accomplished or how much of something should be applied. Procedures explain the step-by-step instructions for completing a specific task. Guidelines are the ones that are not necessarily mandatory. They lay out suggestions and recommended actions or procedures of configuration or other best practices. Baselines: Highly detailed and itemized checklists. A perfect example is the security benchmarks provided by the Center for Internet Security (CIS) for security operating systems and applications.


<li><a href="#">Monitoring breaks down into two main areas that we will discuss separately. One area is the network the ability to identify files, services, and any other information being transferred across your infrastructure. The other area is endpoint monitoring. This involves being able to see what is running and happening on the hosts on the network.


<li><a href="#">Monitoring traffic, recording interactions from machine to machine, and analyzing service logs for HTTP, DNS, and the like all fall under the definition of Network Security Monitoring (NSM). NSM is implemented through multiple different appliance types watching the traffic on the network and either recording what it sees, or specifically trying to point out evil. Most network appliances can pick up the Layer 3 and Layer 4 information (ports and IP addresses) associated with a given session, and sometimes that is enough for determining something is out of order (too many sessions or very large sessions, etc.). But for a complete modern defense, we absolutely must go deeper and be able to extract application layer information.


<li><a href="#">Although network monitoring is a crucial part of our strategy, it is the endpoints themselves that truly hold the data that most attackers are after, and therefore, endpoint visibility is as crucial as network visibility, if not more so. Knowing which programs are installed, which services are running, if any important configuration items have changed, scripts have been run, or auto-start items have been added are all important pieces for keeping tabs on the hosts in your network.


<li><a href="#">Continuous Security Monitoring (CSM) is concerned with configuration and baseline monitoring as well as analyzing and recording information about file and registry changes, processes, autoruns items and the like. It also includes locating and tracking vulnerabilities present in the environment.


<li><a href="#">Log and data centralization is another important topic for a functional SOC. If you are unable to centralize transaction data, metadata, and logs, trying to investigate a potential incident may be an exercise in frustration.


<li><a href="#">The SIEM is what steps in to take care of this for us and provides a centralized repository for all the data. Without a SIEM centralizing logs, if you had to investigate activity for a given IP address, you would have to log into every application individually and try to put all the pieces of an incident together an inefficient way to go about things.


<li><a href="#">In order to effectively defend a modern network, the events on the network and endpoints must be visible. This can be thought of in two camps: Network security monitoring for the "data in motion" on the network as well as continuous security monitoring for the "data at rest" on the endpoints. In information security, the general task is to record events that occur on systems and on the network, flag suspicious ones as alerts, and if confirmed to be malicious, work them as incidents.


<li><a href="#">Events : Any observable occurrence in a system or network. This could be a user logging in, or someone opening a website. Most logs that are collected will fall under the definition of events any observable occurrence. They are simply a log telling us that something happened with no clear evidence that it was bad. Alerts: An event that may be unwanted or unauthorized. An example would be an IDS alert claiming it saw malware command and control traffic.


<li><a href="#">Incidents : A violation or imminent threat of violation of computer security policies, acceptable use policies, or standard security practices. These typically are events that will affect the Confidentiality, Integrity, or Availability of business data. In other words, a confirmed incident. Events that become alerts that are verified as true positives become incidents.


<li><a href="#">The SIEM allows us to parse events and apply threat intelligence, correlate them with other logs, or match it against a blacklist of malicious IPs, domains or otherwise, and potentially upgrade them to alerts. If there's one thing most SOCs struggle with, it's the volume of alerts that are generated. It is extremely common to find organizations buried in a massive amount of alert info and unable to figure out where to start on the pile.


<li><a href="#">We also know that alert fatigue may have played a part in multiple large-scale breaches throughout the years. Both Target and Neiman Marcus made headlines due to the apparent fact that they had missed the alerts that could have keyed them off to their respective breaches.


<li><a href="#">There are multiple types of rules that can be written for an IDS. One is signature or blacklist-based alerts, which will alert on any observed indicators such as URLs or IPs that are known bad, or even URL patterns for things like exploit kits. For these alerts, assuming the signature or indicator behind the rule is solid, then these rules tend to reliably point out evil on the network and should be considered a high-fidelity detection.


<li><a href="#">Anomaly-based alerts are not based on any list of known bad indicators of compromise or patterns, but rather on things that would be defined as unusual within the environment... you will likely find that these types of alerts are more likely to produce false positives than "signature of known evil" type alerts. The best way to improve the fidelity of anomaly-based alerts is to enrich their data with additional context or correlate the activity with other things happening from the same host or user. Doing this may bring to light additional information that can push anomaly-based alerts into the high-fidelity detection territory.


<li><a href="#">The SIEM is a tool that doesn't do any direct data production itself, but instead aggregates all the data from elsewhere. It takes the network sensor's information and endpoint logs and makes them all available for search and visualizing. It is the single best source of information in many environments due to its wide-reaching view.


<li><a href="#">Metrics are one of the most important things your ticketing system can make for you, so incident classification features rank highly in the list of needs for an incident management system. Systems like VERIS (Vocabulary for Event Recording and Incident Sharing) allow for highly detailed classification that explains the delivery vector, motivations, and impact of an incident. Simpler frameworks like the US-CERT Incident Notification guidelines make classification slightly simpler but leave some detail out.


<li><a href="#">Threat intelligence is the analysis of adversaries their capabilities, motivations, and goals; and cyber threat intelligence (CTI) is the analysis of how adversaries use the cyber domain to accomplish their goals. Threat data is the raw information and unarguable facts collected from the environment. In the case of network security monitoring, for example, this would be the logs and packets recorded by our sensors on the network. Threat information is an intermediate step that is all about using analysis to answer a question using the threat data as input.


<li><a href="#">Is a system on the network compromised? We could answer that question using threat data collected from the environment, possibly by looking for signs of malware communication or execution. Threat intelligence requires gathering lots of threat information and aggregating it to make an assessment of some sort. This analysis of multiple bits of threat information drives an organization's security policy, spending, and defensive posture.


<li><a href="#">A threat intelligence platform's main purpose is to store the body of threat information, analysis, and indicators you have collected and then make it available in an easy-to-search way. The database will be manually searched as part of incidents and thus needs to have a user-friendly interface that makes the correlation of data across events easy.


<li><a href="#">The incident management system and threat intelligence platform. These two tools allow SOCs to keep tabs on all the incidents that have occurred and correlate their details together to get a higher-level picture of items that are connected. This capability allows Blue Teams to grow their threat intelligence and, over time, a picture should emerge of the type of threats you face, the tactics and techniques they'll use, and where you should prioritize defensive spending in order to counter the assault.


<li><a href="#">At the highest level, the whole point of a SIEM is to be a centralized collection of logs that we can use in various ways to highlight suspicious conditions and detect attacks and/or compliance issues. To accomplish this, a SIEM must first have all logs in the environment forwarded to it, then parse and store them. Assuming all of this has been orchestrated correctly, we then also want the ability to filter out items of interest with a flexible search language, visualize log data, and enrich fields within our logs to give them more context.


<li><a href="#">Modern networks are generally split up into at least a few zones consisting of rules that will restrict or allow traffic to flow between them based on routing and firewall rules. Understanding these rules, the physical connections that govern the zones, and the points of visibility available on your organization's network is crucial for the defender. Without it, it is impossible to derive where traffic we are interested in can be captured and inspected.


<li><a href="#">When an alert has fired, the analyst often must answer the question "what sensor can I gather the data for this alert from?" Understanding the traffic flow and capture capabilities of your network allows you to answer this question and quickly move to the data required or determine that it may not be available.


<li><a href="#">Zero-trust architecture or micro segmentation. These ideas take the best practice of segmenting the network to its logical limit and essentially make every system an island. If every machine could only talk to other machines that we knew it absolutely had to talk to and was able to authenticate those systems, it would be extremely difficult for attackers to move!


<li><a href="#">Although we cannot stop attackers entirely, we can do our best to slow them down, and the further a network has been segmented, the slower they will progress. Slow progression buys the blue team more time to catch up and stop an incident before it becomes a full-on expensive breach that hits the news.


<li><a href="#">When it comes to capturing data about traffic traversing the network, there are 3 common ways that are used... NetFlow is the first option. It is the lightest weight capture format meant to describe all sessions at a Layer 3 and 4 level (IP and TCP/UDP).


<li><a href="#">Service logs captured by a tool like Zeek (Bro) or Suricata are another option. These tools will look deeper than NetFlow and capture information about Layer 7 metadata as well. They will cut logs for HTTP methods, DNS requests types, hostnames, SMTP connection details and more, plus everything from the NetFlow tools.


<li><a href="#">Full PCAP is the elephant-sized solution of the bunch. The good thing about full PCAP capture is no matter what happens (assuming a connection isn't encrypted) you will have an exact copy of the network traffic and be able to figure out the exact truth. The downside of PCAP is that it takes enormous amounts of storage space compared to the other options.


<li><a href="#">Notice that NetFlow data is indeed high-level. It shows information on IP addresses, ports, frame and byte counts, how long and when the session started, and how long it lasted. This is obviously lacking some detail, but don't count it out as not useful. There are a shocking number of breaches that have been caught using NetFlow alone.


<li><a href="#">Imagine having the information contained in NetFlow, but also including the transaction detail for every session of HTTP, DNS, SMTP, SMB, SSL, and more. These are what the Layer 7 metadata network service logs give you. These tools likely represent the best bang for the buck in terms of ease of collection and ability to detect intrusion.


<li><a href="#">There are certain questions that are the fastest and easiest to answer with NetFlow Who's using the most bandwidth for upload and download? Where is most of my traffic going? Who has the longest running or most network sessions?


<li><a href="#">Layer 7 metadata can get you even further. It's the best for looking for anomalies based on the field data from each specific protocol. Tools like Zeek are capable of parsing out hundreds of fields for typical application layer protocols, and with a SIEM, will enable you to use that data for dashboards, anomaly detection, and alerting.


<li><a href="#">There are many services available both on the internet and through vendors that offer reputation services for domains... The age of a website can be a great additional piece of data to decide whether a site is malicious or not. Many times, attackers will register malicious sites very close to their use in an attack. As a defender, if you aren't sure about a domain and find it was created days or weeks ago, that is another vote for the "potentially malicious" category.


<li><a href="#">One easy heuristic to apply to domains is the apparent randomness and length of the name. Malware often uses random domains to avoid signatures and implements this through what is called a "domain generation algorithm." Each day, the malware will try to communicate with a new, but deterministically generated hostname. The day the attacker wants to send commands to the malware, they will pre-register that domain. Using a domain generation algorithm (DGA) makes it extremely difficult to block the malware by domain or predict when the next command will come or where it will come from.


<li><a href="#">Another option is to look at the Autonomous System Number (ASN). What is an ASN? It associates an IP with the actual organization that owns the IP space, and can be found with tools like centralops.net's Domain Dossier, or any other ASN lookup tool. ASN lookups will not necessarily be able to tell you that something is evil but can be useful to give context and say something is less likely to be evil given its association with a known good organization.


<li><a href="#">A program can come with its own DNS server settings and the ability to do DNS resolution on its own. In fact, this is exactly what happens in many cases with malware specifically to avoid using the organization's DNS server, which may log the requests or block them. Therefore, understanding when a single rogue device is using an odd external server for DNS resolution is a key ability for analysts.


<li><a href="#">If an alert fires for the use of external DNS services, or you'd just like to go hunting for it, there are plenty of places that would record this data.


<li><a href="#">Firewall logs : Any logs (block or allow) from the inside going outbound should record the activity.


<li><a href="#">NetFlow / Network metadata : Any service that is recording Layer 3 and 4 statistics for outbound traffic can be searched for UDP port 53 packets with an internal source and external destination. Host integrity checks: What if malware has changed the system settings for DNS, but the host has not made much or any traffic at all yet? One way we can look for malicious modification of system DNS settings is to monitor them and then alert on any unexpected changes.


<li><a href="#">DNS tunneling uses the admittedly limited ability to transfer data across the DNS protocol, and controlling both sides, makes the content it is sending malware output and file exfiltration, instead of record lookups. The particularly dangerous thing about this attack is that it almost always works. Organizations must allow DNS traffic outbound to the internet from at least one source; otherwise, no one would be able to use the internet. The good news is, if you know what to look for, spotting DNS tunneling is extremely easy.


<li><a href="#">As an analyst, you should understand how and where the DNS traffic is being logged. By "how and where", we mean is the information being sourced from the log file generated on a resolver itself or are you using network extraction?


<li><a href="#">The future of DNS is not yet here, but there are multiple standards proposed to try to make it more secure. Unfortunately, most of those methods also end with the lack of ability for the blue team to monitor queries from the network. Log files will likely become the only way to get information on what domain was requested. One standard starting to emerge is DNS over TLS or DoT. This standard takes typical DNS queries but wraps them in a TLS connection first so that they cannot be tampered with in flight or read by anyone monitoring the wire. As the new DNS protocol standards start to become standard and DNS becomes encrypted, network collection will likely need to be replaced with service log collection from the DNS servers.


<li><a href="#">Most of the HTTP traffic you analyze will consist of GET requests, which are focused on retrieving the file specified in the URL. One of the key items to remember about GET requests is that if data is going to be sent using the GET method, it is almost universally done with GET parameters. This means that the GET method should only be used for small amounts of text, and never for sensitive usernames or passwords. Login information, files, and other larger information is typically passed using the POST method.


<li><a href="#">The POST method is the second most important HTTP method to be familiar with, as it can be used to send arbitrary amounts of data in the body of an HTTP request. It is commonly used for sending larger amounts of information that wouldn't make sense as a GET parameter, for sensitive information, and for data from web forms.


<li><a href="#">Most compromises have at least some HTTP-based component to them or will at least require frequent outbound communication. Knowing the collection sources available to you along the path that traffic must travel to the internet will allow you to center in on what sensor has the information you require.


<li><a href="#">The new standard for HTTP/2 has been released and it is extremely different than previous versions. Most changes have to do with making connections more efficient and enabling privacy with encryption by default. This is great from a user perspective, but unfortunate for network security monitoring as analysts.

<li><a href="#">Being able to view request and response headers and immediately notice if anything is suspicious is an essential analyst skill since so much malware tries to hide running over protocol compliant HTTP transactions. While this is easy now, like DNS, unfortunately the future capabilities for HTTP analysis is somewhat uncertain. New standards with mandatory encryption are coming down the pipe, which will undoubtedly make analysis more difficult and will obscure all but decrypted traffic from view.


<li><a href="#">One of the easier automated ways to check if a URL may be malicious is to check its reputation on one of the multitude of websites that track malicious webpages. The tools listed on this slide are some of the bigger and more well-known names for doing reputation checks, but there are numerous options beyond just these.


<li><a href="#">VirusTotal : The first stop for most analysts. It will check a URL with several URL scanning suites. It's probably is the best place to go for a breadth of opinions aggregated from many different engines. The downside of the VirusTotal URL check is that it doesn't offer many specifics on why any particular engine thought a URL was bad or not, just the good/bad determination.


<li><a href="#">URLVoid offers IP resolution details, basic domain info, and shows any matches for your entry from multiple domain blacklists like VirusTotal. The difference between URLVoid and VirusTotal is the blacklist sources, which are significantly different.


<li><a href="#">If URL reputation services don't offer the detail you need, you can step it up to a sandboxing tool. These sites will render the page in a virtual machine, look for signs of malicious activity, and record what happens on the host that opened the link in case any suspicious changes are made, or processes are spawned.


<li><a href="#">In general, hybrid-analysis.com and any.run are highly detailed and more malware file focused and therefore have great heuristic checks for what is done to the host, but the UI is not as focused on network connections as some other sites. URLscan.io is a bit more network centric and offers more detail on the actual HTTP transactions that occurred, allowing you to see individual requests and responses and download each one.


<li><a href="#">When quick automated methods like sandboxing, screenshots, and reputation checks don't give you the answer you're looking for, manual analysis might produce the results you seek. Looking at headers for HTTP transactions take a bit of time to get used to, but after enough samples of good and bad, you'll start to get an intuitive sense of what makes a transaction suspicious.


<li><a href="#">Since at compilation time malware authors must select what User-Agent, if any, the malware will use when it communicates, it is likely that eventually this may lead to exposing it. A frequency analysis of all the User-Agents on a network can help highlight discrepancies and be a good starting point for generating investigation leads for hunt teaming.


<li><a href="#">For historical reasons, almost every browser will now start with the term "Mozilla 5.0" in the User-Agent. 1 If you see anything other than Mozilla, you are likely seeing an old or non-standard browser client (Opera) or other non-browser software making the transaction.


<li><a href="#">One of the best ways to surface odd user-agents is to use your SIEM to sort unique User-Agents seen on your network by count and look at the ones with very little usage. This is called "long-tail analysis" and is often used during threat hunting activities to identify anomalies in large data sets.


<li><a href="#">Cookie headers can be used for hiding evil as well. Using the cookie field for data besides cookies is a clever technique for attackers to use because the sensitive nature of cookies means they do not show up in most logs. That means finding malware command and control or other information inside the cookie field will likely involve having a reason or tipoff to pull up the cookie field data and look at it in the first place.


<li><a href="#">Base64 encoding is a concept that must be covered at this point because it is an encoding method that is often seen in HTTP transactions both good and bad. Base64 was first used as a way of encoding all possible 8-bit byte values into a 6-bit ASCII printable format so that content beyond basic text could be sent over email. When it comes to base64, you should be able to know it when you see it, as well as realize when it may be out of place. Many legitimate web applications use it for fields in cookies and other authorization mechanisms, but malware often uses it as well to transfer files and perform command and control.


<li><a href="#">It is important to be able to quickly spot base64 encoding as it is also commonly used by malware to transfer binary data over HTTP, SMTP, and other channels as well. Using base64 encoding both helps attackers avoid signature-based detection and obscure what data is actually being transferred. Note that it is possible to encrypt data, then base64 encode it, so decoding a base64 value will not necessarily yield an immediately interpretable result.


<li><a href="#">If you come across the PCAP of an HTTP transaction where a file was downloaded, it's easy to extract the file with Wireshark for further analysis. To carve a file out to your hard drive, simply go to File, then Export Objects and select HTTP (this also works for TFTP, SMB, IMF, and DICOM files). In the HTTP object list, select the transaction for the download then hit Save. After the file is carved out, it's easy to run a virus check, signature verification, or hash calculation that can be run through VirusTotal.


<li><a href="#">If a piece of malware is going to use HTTP as a mechanism for command and control and the adversary wants their remote shell to be responsive, there will need to be repeated GET/POST requests that send new data or check in to ask for new commands. Some malware POSTs data multiple times per second; others may only ping out once a day as a connection check. There are tools such as RITA from Black Hills Information security that is designed to find periodic signals, no matter the distance between them. So just because it's infrequent doesn't mean it can't be caught!


<li><a href="#">Since most internet communications involve the use of hostnames in order to make programs more robust and sites easier to remember for humans, any time a hostname is not used, suspicion should be raised. How do we know when a site was contacted without a corresponding hostname? The Host field in the HTTP headers can give us that information.


<li><a href="#">The traffic pattern for a user hitting an exploit kit is not always exactly the same. Each exploit kit implements the profiling and redirection a bit different, but there are some general trends across them. One is that the kit often involves multiple HTTP requests across various domains and IP addresses. These domains are often suspicious looking in that they have non-sensical names.


<li><a href="#">Fortunately Snort and other IDSs do a pretty good job at keeping up with the URL patterns for infections and can often identify a user hitting a potential exploit kit. If there is any doubt, a quick glance at the traffic should be able to narrow down whether an alert for exploit kit activity is a false positive or not.


<li><a href="#">Over the last few years, the prevalence of exploit kits has decreased immensely. There are multiple reasons for this. The combination of better browser and plugin development practices combined with the reduction of attack surface from less Flash and Java usage has led to a drop in their effectiveness.


<li><a href="#">Encryption is great for defending individuals from attackers snooping on their traffic and performing man-in-the-middle attacks to alter traffic, but it unfortunately blinds the security team as well if measures are not taken to deal with it. Most organizations are now seeing well over 50% of their traffic using SSL/TLS connections. This means performing network security monitoring requires either a shift in tactics, or the ability to decrypt the connections.


<li><a href="#">Even when network traffic becomes completely opaque, that doesn't stop us from gathering information directly from the endpoint. As encryption percentages increase, it is likely that this will be one of the primary answers to the loss of network visibility.


<li><a href="#">First, the client must connect to the decryption device and let it know what site it would like to connect to. The decryption device will then, on the fly, create and sign a certificate for that site issued by the organization level CA and return it to the client's browser, which will only accept it because the organization's root-level certificate is also present. The decryption device then, on the other end, connects out to the website in the same fashion the client would if the decryptor weren't in the way, establishing a secure connection using the certificate the website normally issues. The decryption device has effectively established itself as a man in the middle with one secure connection on the client side using the dynamically created organization CA-signed certificate, and another on the webserver side using the site's normal certificate.


<li><a href="#">It's what happens in between the two different encryption points that allows the security team to break the tunnel and peer into what's inside. During that time of decryption, the SOC can use all its normal analysis tools for sandboxing files, running IDS/IPS signatures, metadata collection, and sending logs to the SIEM. This lets the user effectively keep their secure connection, but still allows the blue team to monitor what's going on!


<li><a href="#">The next big standard in SSL/TLS is TLS1.3. The new standard will be a large shift in strategy for network security monitoring as it will break many of the tools and methods we currently use for encrypted traffic monitoring. For one, all cipher suites that do not provide "perfect forward secrecy" have been eliminated. That means that we cannot simply decrypt SSL traffic by giving an appliance a copy of the private key used for certificate signing. Each session will get its own ephemeral key.


<li><a href="#">The user's email client, generically referred to as a Message User Agent (MUA), is where a user interacts with their mailbox, writing and sending a new email and reading what they have received from others... They have the job of taking the email from the user and submitting it to the specified SMTP server for the user's email account, as well as displaying email that has been received for the user. MUAs often use SMTP protocol to submit emails to the source server, but not always.


<li><a href="#">The source SMTP server receives the email from the MUA and begins to relay it. These servers are referred to as Message Transfer Agents (MTAs) in the SMTP standard. The source organization may have a series of MTAs the email goes through on its side, with the last one being responsible for transferring the email over the internet to the server listed in the MX record of the receiving organization.


<li><a href="#">Once the destination MTA receives the email, it must store it for the user who it is intended for. When the destination server performs the email sorting and storage, it is acting as what the standard refers to as a Message Delivery Agent (MDA). At this stage, the receiving user's MUA reaches out to their MDA and checks for any new mail that has been received, completing the mail delivery.


<li><a href="#">For monitoring purposes, we want to be clear on what type of traffic occurs at each stage, and how the relay of email between these components modifies emails headers, leaving a trail we can analyze.


<li><a href="#">After the MUA submits the email to the first MTA, all the subsequent interactions, up until the receiving organization s MDA receives the email, will utilize the SMTP protocol. It is the SMTP portion of the interaction that we are interested in as these are the interactions that leave a record in the message trace headers of the email.


<li><a href="#">If you want to see an email in its native text form, including all the metadata added by the MTAs an email has passed through on its way to your inbox, pull up the view that shows the original email source. In this view, you should see both trace and message headers, as well as the body of the email. Items in this section include the multiple MTAs the message passed through, details that can be used to tell if a message is likely spam or a spoofed message, which mail client was used to send the message, and, in some cases, even the IP address and hostname of the sender.


<li><a href="#">A look at the original source text of an email on the receiving side shows there are 3 main sections: The trace headers, the message headers, and the email body. The trace headers are added not by the sending client, but by every server that touches the email in its path from sender to receiver, and they indicate information about the hosts it has passed through. The message headers and body are created by the client MUA or the submission MTA.


<li><a href="#">The thing to know about trace headers is that they are written in reverse chronological order each server adds its own information on the top of what is already present when that server receives the email.


<li><a href="#">Received headers are typically laid out in the form of "Received: from (source MTA and IP) by (receiving MTA and IP writing the header) ; [date]". This means if we read the Received headers from bottom to top, if the email was no spoofed, we should see a continuous chain of matching received "from" and "by" hostnames and IP addresses.


<li><a href="#">To combat the lack of authentication in email, several standards have emerged over the years. The three standards with the most adoption are SPF, DKIM, and DMARC. SPF is a path-based technology that verifies mail source based on IP addresses and hostnames. DKIM is a signature-based technology that aims to prove where a message came from by having the sender's gateway MTA sign message components and publish a public key in DNS that can be used to verify the contents. DMARC builds on the previous two by taking the verified information and enforcing the claimed identity in a message's from line so that the multiple fields possibly used for a source address in an email cannot be used to fool the user.


<li><a href="#">How SPF works. Every incoming message to the receiving border MTA has a claimed domain it came from (from the EHLO message and FROM address), and the actual IP address that made the connection to the MTA. The receiving MTA then takes that domain name and does a DNS lookup for a special record that contains a list of sources that can send email for that domain.


<li><a href="#">When the border MTA receives a message and does the DNS SPF record lookup, it must tag the message with the conclusion. According to RFC 7208, SPF checks must return in one of the above results and should be interpreted as follows:


<li><a href="#">Pass: A "pass" result means the client is authorized to inject mail with the given identity.


<li><a href="#">None: With a "none" result, the SPF verifier has no information at all about the authorization or lack thereof of the client to use the checked identity or identities.


<li><a href="#">Neutral: A "neutral" result indicates that although a policy for the identity was discovered, there is no definite assertion (positive or negative) about the client.


<li><a href="#">Softfail: A "softfail" result ought to be treated as somewhere between "fail" and "neutral"/"none". The [SPF record indicates] the host is not authorized but is not willing to make a strong policy statement.


<li><a href="#">A Fail: A "fail" result is an explicit statement that the client is not authorized to use the domain in the given identity.


<li><a href="#">Domain Keys Identified Mail (DKIM) is another mail source identification scheme but instead of being based on the IP of the source of the email, it is based on digital signatures. To apply a DKIM signature, a sender of signed email must first select which portion of the email they will sign. This is called the selector, which can be the body of the email, the headers, or both, or there can be multiple selector options. Once the selector is chosen, the domain owner must generate a public and private key the emails will be signed with and publish that key in a DNS TXT record so that all receivers of that organization's email can find a copy of it.


<li><a href="#">Once the setup is in place, every time an email is sent, the selector portion of the email is turned into a digital hash and that hash is then encrypted with the private key that matches the public key placed in the DNS entry. To be clear, this step is performed by the MTA sending the email, not the client sending the email. It is one key for the entirety of all email leaving the organization, applied centrally. Once the email is received by another organization's email server, that server can then see the domain and selector combination for the message and query DNS for the respective public key. The server then hashes the same section chosen in the selector for themselves and compares the results to the version transmitted in the email encrypted under the private key. If the two match, then the receiver can be sure the email was sent from the organization's infrastructure.


<li><a href="#">To weaponize email by abusing the multiple from address issue, attackers would send an email from one address, while crafting the RFC5322 From address to say another, friendlier looking address.


<li><a href="#">To pass DMARC checks, 2 things must happen, and email must pass the domain owner-specified SPF or DKIM checks and be "aligned." The meaning of being aligned to DMARC depends on if SPF or DKIM or both are used.


<li><a href="#">1. For SPF, being aligned means RFC5322.From header (the one inside the mail content) must match the RFC5321.MailFrom header (the MAIL FROM line in SMTP). The RFC5321.MailFrom line is the one also used for SPF checks.


<li><a href="#">2. For DKIM, the RFC5322.From header must match the domain in the "d=" field passed in the message's DKIM header signature.


<li><a href="#">If a domain has SPF, DKIM, and DMARC set up, and all checks pass, receivers of email from that domain can be very certain that organization was the true originator of that email. Be careful, though this does not mean the email is safe. It just means that it's not spoofed. If an employee's machine has been compromised and an attacker can send "real" email from their account, it would still pass all of these checks.


<li><a href="#">Headers that were created by MTAs before your border MTA are out of your control and should not be trusted. They may be acting maliciously by adding false data. Read the headers carefully as SMTP was designed very naively and has only had security features bolted-on afterward. Many of those security features only work when companies opt themselves in. Realize that without SPF, DKIM, and DMARC set up, it is very easy to send falsely labeled email in several ways.


<li><a href="#">Setting up DNS records for SPF, DKIM, and DMARC is an outstanding way of preventing attackers sending email labeled as if it were coming from your organization but will not do much to directly prevent spam coming in to your own organization (minus the important case of attackers trying to spoof internal email).


<li><a href="#">Why is SMB interesting to an attacker? Because the Server Message Block protocol enables an attacker on a remote machine with an administrative level username and password of a victim machine to connect to it, execute any arbitrary command, and interact on a command line. This is a key point you MUST understand as a defender: the capabilities of SMB make password sharing one of the deadliest mistakes in enterprise networks!


<li><a href="#">The most important thing to know about SMB protocol versions is that you should make every attempt possible to lock down usage to only the newest versions. When establishing an SMB connection, both client and server will send the list of versions they speak and use the highest one supported by both sides. SMB version 1 is especially bad and should be disabled. Ideally, SMB2 should be turned off as well. Without SMB2, most of the tools from the Equation Group released by the Shadow Brokers would not have worked.


<li><a href="#">SMB attacks tend to fall into 2 camps legitimate login credentials used with a tool like PSExec, or an SMB protocol service-side exploit. An attacker may try to sneak into a building and plug a device into the company network, and when this happens, if there is no Network Access Control to stop the device from connecting, it must be detected in some way. Since DHCP servers often will log the hostname, IP address assigned, and MAC address of the device that received a lease, its logs act as a great record of the devices on your network.


<li><a href="#">The MAC address OUI is another field that can be interpreted for rogue device detection. The first 3 octets of a MAC address make up what is called the Organizationally Unique Identifier, and tells you, if the MAC address is not intentionally spoofed, who the manufacturer of a device is. This is useful because if you know all laptops should be Dell devices, the presence of a Lenovo or HP OUI in a MAC address could set off alarm bells.


<li><a href="#">Remember, none of these methods are foolproof. The hostname and MAC address of a device is a self-reported field that the device itself can set. A clever attacker that knows your PC naming scheme and typical MAC addresses may disguise their device by cloning something that already exists or making a similar name / MAC address. Nonetheless, it's much better to at least be able to catch some rogue devices.
<li><a href="#">Can pings be used for evil? You bet! Like most protocols, there is a space within a ping packet to place arbitrary data. So, as with using any protocol for exfiltration, instead of the standard payload, an attacker can choose to pack the ping payload with stolen files, passwords, credit card numbers, or anything else. Each ping has 65507 bytes of space that can be used for anything, if the attacker has the tools required to craft the custom packets.


<li><a href="#">If we have a network that doesn't use FTP to transfer any files internally or outbound, it's very easy to keep tabs on anyone's attempt to do so. If there are legitimate uses of FTP within the network, it's easy enough to write an analytic to exclude those uses and alert on anything else. This can begin to highlight any illicit use of the protocol and potentially stop a highly expensive data breach in unexpected ways.


<li><a href="#">The best defense we have as the blue team is a strong monitoring policy and the ability to detect anomalies. Anomaly detection can be complicated, which is why having a well-defined network with a traffic flow that is known can help bring order to the chaos. If you can say which protocols should be used in which areas, coding analytics to find out-of-place connections becomes much easier.




<li><a href="#">Since logs are one of the most frequently used items in a SOC, it's crucial that you learn how Windows and Linux logging works, and how that affects what we can do and how we can search them as an analyst. Since logs will be the key item in identifying the attacks we've discussed, log collection, enrichment, and interpretation are of special significance for those in the SOC.


<li><a href="#">While we can tell protocols, domains, and destinations from the network traffic, endpoints hold the information that allows us to tie it all together. Questions like which process created the malicious traffic, which user is compromised, and what was going into that encrypted tunnel can be answered with ease if the right endpoint data is collected. Investigating exploitation, code execution, persistence, lateral movement, and data theft are all primarily going to leave artifacts best seen with sources from the endpoint; therefore, collecting and understanding our attack patterns, and endpoint logs and defensive tools that catch them will be central to our effectiveness.


<li><a href="#">Exploitation can be accomplished through a traditional exploit, social engineering tactic, or plain login through credential compromise.... if a buffer overflow exploit is run against a program such as Adobe Reader, the code that runs post-exploit will have no more permission than the program (Reader) that was exploited itself. This means that if the attacker has exploited a process running with limited credentials, then the impact may be near zero, but if the attacker has compromised a service or program running as root, the consequences may be dire. This is the reason that security practitioners constantly strive to reduce privileges programs and services run to the absolute minimum possible... In the case of Windows especially, running as admin allows very simple avenues to steal passwords as well, which compounds the need to run as a limited user.


<li><a href="#">When it comes to exploits, they come in two different flavors client-side and service-side. While service-side exploits have been more prevalent in the past and continue to exist today, they are becoming less and less common. They are called "service-side" because the attacker is reaching out to a listening service on the victim machine. A key difference between service-side exploits and client-side exploits is that service-side exploits do not require user interaction and, therefore, they can be tried repeatedly, likely without the user noticing anything is happening. This makes them more reliable and preferable, when they do exist. Although they are very useful for attackers, their downside is a simple firewall block for that port will stop them from occurring, which means service-side exploits will not be available unless the machine has open listening, accessible ports for the service.


<li><a href="#">Client-side exploits , on the other hand, do not necessarily rely on network traffic for exploitation. Some client side exploits may be delivered over the internet such as browser exploits, which rely on the user going to an attacker-controlled website, but others can be files that contain exploits such as PDFs or Word documents.


<li><a href="#">Post-Exploitation Tactics : Those activities are helpfully broken down in the relatively new and continuously evolving ATT&CK (Attacker Tactics, Techniques, and Common Knowledge) matrix by MITRE. ATT&CK is an effort to enumerate all the post-exploitation tactics (high level activity) the attackers may engage in and the specific techniques to accomplish each tactic. Each item in the matrix has a thorough explanation, citations of when it was used by different groups and malware families, and ways to prevent and detect it... Although, in practice, not all items can strictly be covered as well as others, it makes for an outstanding guideline for developing a defensive strategy.


<li><a href="#">After successful exploitation often comes code execution. In fact, the initial code to execute is bundled as the payload of the exploit so that if the exploit does function correctly, it can tell the host what to do next. At this point, the type of code that is often run would be code that could either spawn a typical command shell or perhaps even a custom command shell with attack tools built in, like meterpreter. From this point on, the attack will then move to installing malware and maintaining persistence.


<li><a href="#">Code execution is one of the most important parts of a compromise because without the ability to run tools or scripts, it would be very hard for attackers to do anything of use. This is why tools like whitelisting are so effective at stopping attacks. When the adversary is restrained from running arbitrary code, or even runs into the whitelisting tool at least once during an attack, the security team will immediately know something odd has happened and get a chance to respond quickly.


<li><a href="#">A tactic common to many intrusions is persistence, or, the adversary using some technique to ensure that they have the continued ability to control the machine over time... Persistence doesn't have to be attempted in only one way. There may be a registry key startup item for malware to run every time the system is booted and a scheduled task to check for the existence of the malware and redownload it if it is deleted. With a setup like this, even if the security team finds the original malware infection and registry key, if the scheduled task is missed, the attacker may be able to get back in and continue their mission.


<li><a href="#">Once attackers have gained interactive command and control, they will need to start to learn about the environment they're operating in. This involves enumerating as much as possible about the systems on the network, users and groups in the environment, privileges and permissions for each user and group, file and folder structure on the host, and any local or network services that are being run.


<li><a href="#">Most targeted attacks cannot be accomplished without attaining a state of privilege higher than that of the typical user, but there's an issue... It's difficult to compromise a highly protected account from the get-go. Adversaries will need to find a way to compromise a lower-level account and leverage that account to perform privilege escalation.


<li><a href="#">Many privilege escalation methods rely on misconfigured permissions on files and folders. Depending on the operating system, various methods can be used to modify protected startup items, service executables, or scheduled tasks that can allow an unprivileged user to control what is run by a root account, effectively giving them root/Administrator-level permissions.


<li><a href="#">In order to perform the typical credential dumping attack, the attacker must have already reached an administrative level on the victim machine. That is a requirement because credential dumping requires the privilege levels to read sensitive system files or bytes from memory, which is something only administrators have.


<li><a href="#">One of the biggest things to happen in the Windows attack space in the recent past was the introduction of tools like Mimikatz. When security professionals say that users should not ever run as the administrator account unless they absolutely must, this tool is one of the reasons for that statement. Mimikatz is now a very common credential dumping tool and is one of the capabilities that shows up in breach after breach because it is so effective. Windows... stores the password in the memory space for the process lsass.exe, and it is this functionality that Mimikatz can abuse. It will reach into the lsass.exe process itself, given the capability, and reverse out passwords to the best of its ability.


<li><a href="#">Lateral movement ... can be easy given the wealth of protocols that allow us to connect and control one host from another. While tools such as SSH, PowerShell, WMI, RDP, and others make it fast and easy for administrators to do their job, they also assist attackers in pivoting from one machine to the next, given the credentials to do so. Since lateral movement requires the ability to run code on a remote machine, generally two things are needed to achieve it: Access to the remote machine, and the availability of the code the attacker wishes to run.... When staging malware on the host itself, attacks tend to use infrequently used places such as temporary folders or the recycle bin, this helps them stay below the radar.


<li><a href="#">Often, the final goal of a compromise is exfiltration of data... Given that the target of data theft is often large databases, attackers will need to find a way to not only move gigabytes or more of information, but also must do so in a way that doesn't raise suspicion and ideally doesn't reveal what is stolen if they are caught. Since data is often not able to be sent directly out to the internet from where it is stored, this means attackers must move the data once across the internal network to a staging server or desktop that can reach the internet, find a way out that is allowed through the firewall, and obscure the data such that it can't be identified in transit.


<li><a href="#">The final step is attackers must pick a method for sending the data outbound. This includes what IP address or domain, a destination port, and an application layer protocol to use. If a network is properly set up with a default deny outbound policy on the border firewall (which is unfortunately not the case in some organizations), many of the choices should be immediately eliminated as potential options.


<li><a href="#">It's up to the traffic inspection capabilities of the organization to at least detect their attempts and hopefully stop the upload before it completes. In practice, this is very hard to do as many exfiltration methods will use the correct protocol specified for a designated port (HTTP for 80 for example), leaving detection to rely either on the destination URL or volume/timing of the upload to key off.


<li><a href="#">Post-exploitation stage attacks mean the attacker has gained some level of access to the environment and will be making steady progress toward the goal. Since many post-exploitation techniques are host-centric, using host data will play a crucial part in getting in the way and slowing the enemy down.


<li><a href="#">Step 1 of preventing exploitation is having a true inventory of what is running on your endpoints. Without a list of which services and programs are running, you have no hope of protecting them from exploitation. There are two general ways this information is collected, via inventory systems and via network scanning. Inventory systems are like vulnerability scanners in that they can periodically log in to each machine and create a list of installed software and save it into a centralized database that can be polled. This is a great first step but is a potentially incomplete view of what is truly present. The inventory systems are only as good as the methods they use to enumerate installed software, and depending on how applications were installed, they may or may not get logged correctly.


<li><a href="#">A way to round out this view is to scan each machine over the network to see what listening services are presented to the network... Scans can be done at varying levels of depth and are not just a simple TCP connection. Once a successful connection is made, even without logging in, scanners can grab and interpret banners returned from each server, use heuristics to predict the operating system version, and even run scripts to probe listening services... Combining information from network scans and software inventory systems gives us a great start at enumerating our network's attack surface so that we can protect and reduce it.


<li><a href="#">Vulnerability scanners ... The goal of these appliances is to track all deployed software throughout the enterprise, put it in a database that can be queried, and assist with prioritizing patch deployment. Vulnerability scanners collect vulnerability information from each individual host with either authenticated or unauthenticated scans. Authenticated scans allow the appliance to log in to the host being checked so that its version numbers can be truly enumerated, and the scanner can know for certain what is installed.


<li><a href="#">Anti-Exploitation tools are one of the first lines of defense against malicious files. Since exploitation is the fourth step in the cyber kill-chain and the first host-centric one, it is one of the earliest chances we must disrupt an attack with the victim's endpoint, which is ideal since early-stage attacks are easier and cheaper to recover from.


<li><a href="#">The most widely available anti-exploitation software for Windows is the built-in "Exploit Guard" feature. Exploit Guard is a set of granularly controllable options that apply to several areas of Windows, and let users lock down various applications, data, and network interactions to protect them from common attack techniques.


<li><a href="#">One of the more interesting tools devised by Microsoft for securing credentials is Credential Guard. Available only for the Enterprise versions of Windows 10 and Server 2016+, this is an outstanding security feature that helps prevent attackers from tampering with critical processes and memory sections by isolating them using the virtualization features of the underlying system hardware. Once activated, the process that handles credentials and authentication, the Local Security Authority (LSA) seen on your system as the lsass.exe process, will effectively be separated from the rest of the operating system in what your host hardware considers its own virtual machine. This means even attackers who have gained the highest- level privilege will not be able to use Mimikatz to read the real LSA process memory and extract the password in the typical way, significantly raising the bar for credential dumping.


<li><a href="#">A fully instrumented host firewall can assist at multiple levels of the kill chain and make life extremely painful for attackers. They can stop lateral movement via blocking and detecting attempts to connect to management ports from unexpected internal locations, hindering the attacker's ability to even attempt lateral movement.


<li><a href="#">One of the absolute best moves you can make for endpoint defense is implementing a whitelisting solution. Whitelisting tools function by working off a pre-determined list of known executables and alerting the user of the attempted execution of anything beyond that list. Whitelisting is often implemented in a multi-stage deployment where IT will attempt to learn all executables in the environment and put them on the list before flipping to "block" mode. If they move prematurely, there is the potential for business disruption. In the meantime, most solutions have an "audit" mode, which will allow reports of items that aren't on the whitelist without stopping them.


<li><a href="#">Whitelisting is not a perfect answer for all evils, though. One reason is that it does not stop the malicious files from being transported to the host or written to the hard drive just their execution. Depending on the implementation specifics, there's also several ways it may be bypassed. Malicious scripts are one tried-and-true method for bypassing whitelisting technology. Since many whitelisting solutions may not consider scripts software, attackers can simply make an evil script and run that instead.


<li><a href="#">Code injection is another viable option. The essence of this method is taking malicious code and using Windows capabilities to place it into an already running and vetted process. Since whitelisting will not go back and check once the program has been verified to be on the whitelist, this method can hijack an already known item to make it evil.


<li><a href="#">Hash/signature-based whitelists are very complicated to bypass, but in theory, it can still be accomplished. For signature-based whitelists, the attacker would need to break into a whitelisted organization and sign malware with their private key.


<li><a href="#">Do these bypass techniques mean that whitelisting is not worth doing? Absolutely not. Whitelisting eliminates an incredible number of opportunistic attacks and allows the team to focus on the true threats instead of chasing adware and bots.


<li><a href="#">Host Intrusion Prevention and Detection Systems (HIPS/HIDS) are complementary protection for endpoints that help prevent and detect any changes that might be indicative of compromise. HIPS/HIDS typically use an agent running on each system that utilizes the auditing and logging capabilities of endpoints and enables the security team to write rules for system changes of interest that will be centrally reported to the managing server. Another option would be to monitor for any processes writing ".exe" files into the user's AppData/Local/Temp folder inside their profile folder. This is a common location for malware to run from since it is hidden from most users and can be written to with user-level permissions. A notification rule for the writing of executable content to this folder could be a good step toward malware installation detection.


<li><a href="#">One of the best ways to ensure that privileged accounts are not stolen for use elsewhere is to make it extremely difficult to access them. One of the best ways to do this is to maintain a strict separation of where "normal" account credentials are used and where privileged credentials are used... One of the ways to maintain this separation is what Microsoft calls a "privileged access workstation" or PAWS machine. If dedicated computers or virtual machines are used to maintain this separation, then admin account credentials cannot be jeopardized through a phishing attack or web drive-by download.


<li><a href="#">Endpoint Detection and Response (EDR) solutions products are based around generating a much higher level of visibility of the activities occurring on each endpoint. Through a deployed agent, EDR acts like a flight data recorder for each host, keeping track of all important system activities and the metadata about their operation. Everything from processes and command lines to services, DLLs, files, registry keys, network connections, and the relationship between them all is recorded and can be browsed and visualized by the analyst.


<li><a href="#">Data Loss Prevention (DLP) : Current DLP use cases tend to break down into 3 categories: Non-malicious insiders: Preventing employees from breaking data governance policies such as putting HIPAA data on unencrypted removable media Malicious insiders: Stopping employees who might want to cause the company harm by intentionally breaking policy to steal, corrupt, or destroy data. Malicious outsiders: Similar to malicious insiders but focused on ensuring attackers cannot get a hold of and export sensitive data.


<li><a href="#">The ultimate goal of defense in depth is to force adversaries, even if they progress beyond the perimeter, to slip up and trigger an alert that will allow us to remove them from the network and start the process over again. The more traps we set, the slower they can progress through the environment, and the slower they progress, the more time we must catch them making that fatal mistake.


<li><a href="#">As analysts, it's important to understand the logging collection pipeline. Being intimately familiar with where your logs come from and what they contain is an important piece of being effective at speedy triage and analysis. Windows has a somewhat unique way of writing logs that analysts should understand. First, each log that is written using the OS logging service is written to what is called a log channel. Each individual channel is a .evtx format file that consists of XML records in a binary encoded form. The most common tool for reading .evtx files is the Windows Event Viewer.


<li><a href="#">The Windows Event Log service then takes the message and, using the included metadata, will split it into one of several log channels. Those channels are all written to separate .evtx files in the C:\Windows\System32\winevt\Logs folder and will also show up as an individual icon in the Windows Event Viewer.


<li><a href="#">How do we choose and set which information will make it into the Windows log files? The primary way for operating systems events in Windows is the Windows audit policies. These are granular settings that configure Windows to log on individual events such as logon/logoff, object access, new users, or group management, and whether a log should be written on success or failure of those actions.


<li><a href="#">Another way to collect additional information that many organizations choose to use are third-party programs like Sysmon that add additional logging capability to Windows. Sysmon is one of the best options for adding process creation, network connection, and other logs into a Windows event channel that can be centrally collected and analyzed with a SIEM. When considering which log channels to pick up, many organizations' first instinct is to go for the Security log, and potentially throw in the System and Application log as well. They're right in that the Security log is the most important and that the others are very useful, too, but there are many more useful channels beyond that. To fill in the full endpoint security picture, there are a handful of other log channels that should be considered:


<li><a href="#">The PowerShell operational logs, although large and intimidating, are one of the most important logs to collect. Many advanced attacks stick to strictly PowerShell-based, in- memory-only code that will be impossible to detect with the traditional security channels only.


<li><a href="#">AppLocker : This log channel records AppLocker (whitelist violations), and consists of multiple channels for EXE and DLL, MSI and Script, and Packaged app alerts. Each provides specific information about applications that ran afoul of, or passed checks against the whitelist.


<li><a href="#">Sysmon/Operational : If you have Sysmon installed, it should be a no-brainer to collect this log channel where all events are recorded.


<li><a href="#">In summary, here are the questions you should be able to answer about your Windows log collection to ensure you have a firm understanding of your collection strategy and what logs will be available to you.


<li><a href="#">What is your Windows Auditing policy, and do you have any third-party programs generating additional information? What about scripts or programs that write their own logs outside of the Windows event collection system? What log channels and additional sources of data are you picking up? In your SIEM, do you see the XML fields, the General tab "message" version of the log, or both? Does your SIEM properly parse all the fields?


<li><a href="#">When it comes to event logs, one of the most frequent ones you will deal with are Windows logon events. Login events are stored in the Security log channel under event ID 4624, (4625 for failed logins) and there are many details included for each event. Most of the detail will not be of use to us in a security capacity, but fields such as the account name, domain, logon type, and network info are vitally important to understand.


<li><a href="#">The logon type is another important field to understand. When investigating a possible intrusion, it is important to remember these codes and be able to interpret what they mean about the situation. If an attacker is seen performing a type 2 login, you can likely conclude they have physical possession of the machine as opposed to if you see a type 10 login, which just means they were able to log in over the network.


<li><a href="#">Event ID 4624 is one of the absolute must-knows for any analysis, so it is highly worth your time learning how to interpret this log, and what the different fields mean. Sorting the useful from non-useful event ID 4624 logs will take some extensive filtering beyond just the ID itself. One common first move is to filter out all 4624 events with an Account Name ending in a dollar sign. The PC's name with a dollar sign on the end is how Active Directory refers to the computer accounts and their login activity is rarely of interest, although it may represent the largest volume of logs in an Active Directory environment.


<li><a href="#">Event ID 4648 is another vitally important login item to understand. This is a separate event ID to inform you that someone performed a "runas" command to start a process with another account. While this sort of thing is common in Linux with tools like sudo and su, it is less common to perform in Windows and, therefore, can be a key indicator when an account becomes compromised.


<li><a href="#">Linux logins are again potentially simpler to read and interpret, but more complex to parse in an automated fashion. That's because for one, they are written in syslog format without any kind of standard structure, which makes parsing them with a SIEM hard, and two, the sequence of logging in is broken up into multiple lines of partial information.


<li><a href="#">One of the most important logs you can collect are logs that record every time a new process is created. Ideally, these logs should record not just the process started, though, but what the parent process was that caused it to start, what arguments were used when starting it up, the path, and the hash and signature status of the executable. Almost all malware will necessarily leave some sort of trace in the process log. Whether the environment has whitelisting or not, process creation logs give us an idea of who is running what and can identify when an odd process that hasn't been seen before in the environment pops up.


<li><a href="#">Windows auditing is a fantastic solution for process creation auditing since it is built into all new versions of Windows. Its main benefit is that it doesn't require any extra agents and is likely to immediately work once it's turned on since most organizations will already be collecting the Security log channel.


<li><a href="#">Sysmon is the more detailed option. It requires installation of the driver and service, configuration of Sysmon events to collect, and the ability to pick up an extra event channel. The serious benefit comes in having the option to get multiple different hashes directly inside the log. If this is centrally collected by a SIEM, this means any time a suspicious process is run on a machine, not only will you immediately receive a log of it, that log will allow you to start your hash-based checks without having to interact with the endpoint or file at all.


<li><a href="#">The most important features of host-based firewall logs are that they not only give us network visibility to every single device on the network, but they also let us tie network activity to a process, something your network firewall cannot do.


<li><a href="#">Audit event types are important to understand because if auditing is set up for an object, someone has already determined that it is of higher importance and is worth being watched... The event ID's 4657 (a registry value was modified) and 4663 (an attempt was made to access an object) cover registry and file access auditing and their log messages are straightforward to interpret. Additionally event ID 4660 (An object was deleted) can inform you when files have been removed from the system.


<li><a href="#">Due to their common nature and frequency of install, though, attackers have found that registering a new service with the system can be an effective persistence mechanism. Most users do not search through their list of services to see if everything in the list is a legitimate item. As the blue team, we need to be able to spot when this technique is being used against us. Windows helps us out by registering event ID 7045 when a new service is installed, but it is up to us to collect and analyze these logs for anomalies If you suspect a service might be evil, see if it is installed in a path the system would normally use for something important like this, or is it stashed away in some user's temp folder?


<li><a href="#">The scheduled task is simply a timer that can be set to run a specific command in a delayed and repeated fashion, either locally or on a remote computer. Repeatedly run scheduled tasks are useful for attackers as a means of persistence in that they can be used to reinstall malware that has been found and removed by a security team, or just used to repeatedly start it at a known interval.


<li><a href="#">It's a roundabout way of infecting a machine that may fly under the radar of some security tools since they are using legitimate Windows tools and "living off the land." For this reason, we should be familiar with the Security Event ID 4698 and, if not currently collecting it, ensure that it is added to the auditing policy in Windows. To start collecting these events, the "Audit Other Object Access Events" option must be configured in the Windows Advanced Audit policy.


<li><a href="#">USB devices represent another potential avenue of attack, so we must know how to track their usage as well. In the Advanced Audit policy, there is an "Audit PNP Activity" option that can be enabled that will cut an event ID 6416 event every time a plug and play device is inserted into the system. Mass storage is not the only USB device type that can cause harm, therefore, regardless of the type of USB device being used, we want to know how to detect and interpret the logs.


<li><a href="#">One way to accomplish steady, safe, persistence is to create a new user, ideally in the Administrators group, that can stay present on the system to let them back in if needed. Because of this common technique, we must be able to not only detect when new users are created, but also when they are added to groups. In Windows, the event ID that will identify new user creation is the Security channel event ID 4720. If a security group is modified i.e., a new user is added to an admin group, then there are two potential event IDs that could result 4732 and 4728. Event ID 4732 is created when the user is added to a local group, such as the built-in Administrators group on a single system. Event ID 4728 is created when the user is added to a global group, one that is created in Active Directory.


<li><a href="#">Since PowerShell-based malware is becoming more common, a log that you may have available in your environment if you have set up the policy to pick it up is the PowerShell Script Block Log event ID 4104 in the PowerShell Operational log channel.


<li><a href="#">Once you have a grasp on the general methods attackers use to move throughout the network, the second part is understanding how those methods map to the various audit policies and how to interpret the logs those attacks will generate.


<li><a href="#">After logs are created on the host or appliance they then must be transported to a log aggregator. What happens at the log aggregator depends slightly on the SIEM solution, but, in general, we can say the logs are accepted at a high rate of speed then potentially parsed, cleaned up, enriched, or filtered and ultimately output toward the storage system in a more standardized format.


<li><a href="#">After the aggregator, logs move on to a host that will store and index them. This storage system often does most of the heavy lifting for making logs accessible and searchable. The storage system may also take on the role of the reporting and alerting engine since ultimately these are data searches as well (searches that will take automated action depending on the output).


<li><a href="#">Log collection agents can come from your operating system built-in tools, your SIEM vendor, or a third-party log collection software developer. They are their own separate process that runs in the background and has the sole job of reliably collecting logs and forwarding them over the network to the log aggregator. Agents are a preferred method because they often come with the ability to pick up multiple different log sources easily, filter messages by contents, buffer logs, and send them out of the network in a compressed and encrypted fashion to ensure they are safe and efficient.


<li><a href="#">The other option is agentless collection, which can be used when agents cannot be used or are not wanted on the endpoint. Agentless collection either logs into the machine from a remote endpoint and pulls the data, or a script is scheduled to periodically run on the host that will push the logs outbound to their destination.


<li><a href="#">The preferable logging formats include predictable structure. For structured log formats, there are a few options you most often see:


<li><a href="#">Comma Separated Value (CSV): CSV is the most compact of the structure formats because it contains the bare minimum of formatting. To set up a CSV log source to get parsed by a SIEM, it will need to know the column names and order of those columns.


<li><a href="#">Key-Value Pairs: The key-value pair format begins to be more robust. Data in this form can have fields arbitrarily added, removed, or shuffled in order without parsing being affected. It is a good middle ground between efficiency and dependability.


<li><a href="#">JavaScript Object Notation (JSON): The JSON log is the most robust, including the benefits of key- value pairs, but also allowing nested fields and arrays, something only this format and XML can easily handle. By far, the preferable format is JSON, since ease of parsing should take top priority in any system aimed for detection.


<li><a href="#">Why do we care so deeply about log structure and understanding how SIEMs parse logs? Because fast and accurate searching in the SIEM is 100% reliant on functional parsing. To achieve fast search speeds, the SIEM does not simply save a copy of every log and do the equivalent of a grep when you search for a log. It can do that, but that is usually referred to as a full text log search and is incredibly inefficient. The better way, and what all SIEMs do with varying tactics, is parse the key fields out of each log, and write some of those key fields into a database that can be queried at a comparatively high speed, giving you the fastest possible answers.


<li><a href="#">This brings us to another key issue for SIEM searching scoping your search. Any time you want to run a SIEM search, you should be as specific as possible about the type of log that contains what you re looking for, as well as the field the result is expected to be in.


<li><a href="#">The catch here is that in many SIEMS, not all fields are indexed into a quickly searchable database. The reason for this is that it would be infeasible to do so in some SIEM architectures as the database sizes and resources it takes to do this for every field in every log would bog down the system. Therefore, if you have a SIEM like this, to be most efficient with your SIEM and searching, you should:


<li><a href="#">1.Ensure that all the most frequently searched for fields like IP, username, etc., all do get indexed. You should know which fields do and do not.


<li><a href="#">2.Only run a search for indexed fields where possible. Including any non-indexed fields mean the SIEM will have to revert to effectively going through logs line by line, which will slow you down.


<li><a href="#">The end goal, when you look at a log, is to not only immediately recognize the source of that log but to know how to pick out the fields that are important, recognize the format of it, and realize when it may or may not be parsed correctly, so that your search can compensate for that fact. The only thing worse than not collecting a log is collecting it, but not putting it to use because you can't find, understand, or parse it.


<li><a href="#">Log enrichment is a generalized term for what the SIEM should do with a log after it has been received and successfully parsed. Simply, it is taking the originally included data fields and using them to look up and pull in more data not originally included in the log to give it more context. This is another reason parsing is incredibly important. If logs can't be parsed, enrichment can't be applied and your ability to triage quickly is destroyed.


<li><a href="#">Beyond enrichment, another method for making logs easier to find and understand is normalization. Normalization usually occurs in a couple of ways: One way is field name normalization, and a second one is categorization. Field name normalization is taking all the disparate naming schemes that all your log sources use for a single item such as source IP and making sure that they are all searchable under a common term. This can be done upon ingestion by renaming the fields in flight, or after the fact as a secondary field, preserving the original name. Regardless of how it happens, it is an important piece of ensuring you can find the data you're looking for.


<li><a href="#">Many SIEMs have some facility to perform what is called categorization. With categorization, it becomes possible to find logins in a generic way that does not require analysts to know Windows event ID codes or other specifics about how a login event would look. They simply must know there is a category for login.


<li><a href="#">The final stage of the pipeline is log storage. While SIEMs perform this in radically different fashions depending on the storage and database architecture, in general, we can say that most solutions tend to store like with like when it comes to logs. What is important to know is that when you go to search the SIEM for a log, you will likely need to specify the part of storage. Although the specification may not be necessary, not specifying will mean that the SIEM will attempt to find your data in all possible locations, which obviously is super inefficient and slow.


<li><a href="#">If you think about when a log is most likely to be useful and interesting, in most cases, the answer is right after it is written. New logs are used for alerting, threat hunting, triage, and incident response tasks of the day. As they sit in the SIEM and age to become days, weeks, then months old, the likelihood of them becoming relevant to an investigation only decreases. Therefore, keeping our most recent logs on "hot", fast storage and aging data onto larger, cheaper "warm" spinning disks makes sense from a resource optimization perspective.


<li><a href="#">Analysts should be familiar enough with the collection pipeline to explain what happens to logs at each stage so when it comes time for them to hunt down a specific log, they are not thrown off course due to a lack of clarity on how the system works. In this regard, understanding agents, formats, enrichments, and normalizations applied is a large step in the right direction.


<li><a href="#">The first attempt you should make to quickly identify a file on a Linux machine is the file command. This command has a library of known file formats and can quickly identify, based on the bytes in a file, what format it may be. How does file work its magic? Magic bytes, otherwise known as the file signature! Almost all file types have a standard that governs how their bytes must be structured for the program that reads that file type to be able to successfully interpret it. One of the common features of these structures is a signifier in the first few bytes of a file that can positively identify the file as a certain format.


<li><a href="#">There are tools that can identify nested files based on magic bytes and cut them out as well. The difference is they are programmed to attempt to match magic bytes at any point throughout a file instead of just at the beginning like the "file" command does. Programs like foremost, binwalk, and scalpel are examples of tools malware, firmware, and forensics analysts may use to automatically extract files nested within each other. Becoming familiar with the magic bytes of the common file formats will help us identify potentially malicious attachments or downloads, no matter their name.


<li><a href="#">There are "plaintext" files, files that only have "printable" characters, and "binary" files, those that have bytes that cannot be interpreted as text for one of the standard encodings. Running the Strings program in Linux will print out all continuous runs of printable characters in a given file by attempting to decode it with various encoding standards. Running it against a binary file such as an executable may find sections of printable bytes such as metadata, URLs, and other items of interest depending on the file type.


<li><a href="#">Running strings with the "-e" option will tell strings to look for characters not only encoded different, but also with different byte order (little endian vs. big endian whether the least or most significant byte of the symbol comes first). In summary, when running strings, do not just give up if the ASCII encoded version does not yield results. Try the other encoding options as well.


<li><a href="#">The "strings" of a file are incredibly useful for a variety of reasons. Since all malicious programs will need to modify certain files and registry keys and contact domains or IP addresses, these values must be hard-coded into the program in some location. Looking at the strings for an executable is a great way to identify all the files malware may drop and all the sites it may contact. This gives you a fast and easy way to get ahead of it by forcing it to give up its secrets, and proactively blocking or detecting the changes before the malware has a chance to cause damage. When we analyze the strings of a program without running it, this is called " static analysis", the alternative being analyzing malware by letting it run in a virtual machine or sandbox, referred to as "dynamic analysis."


<li><a href="#">Files are merely a string of bytes meant to be read in a certain way. Sometimes, those bytes represent characters from the ASCII or Unicode encodings (plaintext files), and sometimes they do not map to any printable characters and, instead, represent machine code of a program, or a proprietary compressed file format (binary files).


<li><a href="#">If you do have to investigate a file for potential evil, where should we do it? The obvious answer is "not on our main PC." It's just too risky to handle any type of potentially malicious file on the computer we conduct day-to- day business with. The better answer is in a virtual machine or, at the very least, a dedicated analysis machine.


<li><a href="#">Use an operating system that cannot possibly run the code used for infection. That means using Linux for analysis of files you suspect to be Windows executables, PowerShell scripts, visual basic scripting, Office documents and batch files. None of these file types function in Linux.


<li><a href="#">On the flip side, if you are investigating potentially malicious bash scripts, ELF binaries (the Linux equivalent of an EXE), or some other Linux specific format, you can safely use Windows to look at their components without fear of infection since their contents mean nothing to Windows.


<li><a href="#">For files that potentially cause danger in all operating systems JavaScript, PDFs, etc. Linux would still be the recommended operating system for analysis. Why? Most attackers using these file types are sending them with the assumption of the victim running Windows. That means their infection chain at some point will eventually affect Windows only. The right way to move a file is to ensure it cannot be found by AV, IDS, or accidentally executed on the collection machine. The most common way to do this is to put the malware in a compressed and encrypted archive with the password of "infected."


<li><a href="#">Successful exploit delivery can be thought of in two pieces: The exploit itself and the payload. The exploit is the code that causes the program to go into an undefined or unexpected state. This may be an unexpectedly long input that causes a buffer overflow, a path traversal vulnerability in a webserver, or invalid input bytes that cause the program to go into an unstable state. If the exploit works, then the attacker will have the ability to cause the program to take action the programmer did not intend. It is this second part that determines the impact of the exploit. This part is referred to as the payload what the attacker that has created the exploit wants the program to do if it is successful.


<li><a href="#">When analyzing a file, your job is to find both these pieces. You need to identify not only what the exploit is, but what will occur if that exploit lands... In order to remedy the situation and ensure that it doesn't happen in the future, you want to block the file both at the exploit level by patching or hardening the application that was exploited as well as detecting and blocking the malicious action that was taken as a result of the exploit.


<li><a href="#">Executables are one of the most common forms of malicious files. Although you may be aware of their dangers in their common forms, do you know of all the other formats they might appear in? We all know .exe files can be malicious, but what about .acm, .ax, .cpl, .dll, .drv, .efi, .mui, .ocx, .scr, .sys, .tsp, .msi, .msp, .sfx, .sea, .jar files? You very likely have an email and proxy block set up for the exe format, but don't think your job is done there. There are many more obscure formats that have been weaponized in their place.


<li><a href="#">Scripts are another weaponized filetype for several reasons. First, they are slightly lower profile than malicious executables, so in organizations without proper email sanitization, they may sail through filters untouched. Scripts also very rarely have whitelisting tools stopping them from running, which can make them a great technique for whitelist bypass. The wealth of languages makes choosing one easy. Even if attackers can't email in a .bat file, they may be able to slip by a .js, .ps1, or .vbs, or .py file. Using the built-in Windows tools cscript.exe and wscript.exe, users can simply double click a .js or .vbs file and execute malicious code just as easily as any other executable.


<li><a href="#">To successfully find autorun macros in .doc files, the spam appliance must be able to tear apart and interpret Office files to look for autorun macros or run all received items in a sandbox a fairly serious undertaking for mail filtering. Office document-based file infection is one of the primary methods for initial delivery and exploitation in attacks, and for these reasons, it is likely to stay that way for a long time.


<li><a href="#">An example of an unsuspecting file type that is commonly weaponized is the Windows shortcut link (.lnk). These have shown up in phishing and have been used for multiple exploits in the past as well. In fact, one of the zero-day exploits used in the Stuxnet attack was based on .lnk files. You may not be aware of it, but .lnk files are not only capable of delivering exploits, but can be used as a backhanded way of running a script as well! Attackers can specify a link to a program to run and the arguments to supply to that program, so a simple malicious use of this will load cmd.exe or PowerShell and simply supply a command line argument of encoded commands that will download and execute malware!


<li><a href="#">Here are some simple, surface-level checks you can use to quickly try to determine if a file may be good or bad.


<li><a href="#">Looking up the file's hash in an online malicious file database like VirusTotal For file types that can be signed, checking for the presence of a digital signature If the file is a document, simply opening it up in a safe virtual machine or putting it into a sandbox that will give a screenshot can easily reveal an answer Basic manual static analysis check strings. If you know what a file's bytes should look like, it will become quick and easy to spot an anomaly. Things like the magic bytes for another file type nested within the file, or scripts where they don't belong are a dead giveaway that something out of the ordinary may be hidden inside. If you run the code in a sandbox, what changes are made to the environment? Are new processes spawned? Sites contacted? Is any apparent action attempted at the onset of opening the file? (autorun Office macros or PDF docs).


<li><a href="#">Hashes are a fast and simple way of identifying known bad, and should be one of your go-to moves in trying to triage a mystery file. They also have the bonus property that you can give a file hash to any website, and unless they have already seen the file that created that hash, they will have no idea of any of the content that produced it. This is good in that we will not be leaking any sensitive information if we submit file hashes to VirusTotal.


<li><a href="#">One way we can start to get a better assurance of a program beyond the hash is the presence of a digital signature. Signed programs not only tell us that they haven't been altered since their creation; they also include a cryptographically strong guarantee of who created the program itself. Just because someone claims the software, however, does not mean that it's good, but it can be a stronger indicator in the right direction. Certificate issuers are supposed to do their due diligence to be satisfied that the person applying for the certificate is truly who they say they are before issuing and signing a certificate for them.


<li><a href="#">Once the developer has the certificate, they can sign their program for Windows with what is called the Microsoft Authenticode signing algorithm. The Authenticode algorithm hashes as much of the content of the program as possible, then encrypts the hash with the private key given to the developer by the certificate authority and appends all of the signature details onto the end of the program. When the user wants to verify the signature, the public key, which is tied to the developer via the signature from the certificate authority, can be used to decrypt the hash, proving it was signed with the corresponding private key.


<li><a href="#">One of the best tools for quickly verifying the Authenticode signature of an executable file is the Sysinternals Sigcheck tool. Not only will Sigcheck verify the signature and hash of the file, it will also print out all the developer's details from the signature, several formats of hashes, an entropy measure, and even check VirusTotal results for you.


<li><a href="#">In theory, the only way to weaponize a signed binary is to either generate a certificate yourself and sign your malware, generate a hash collision with an evil program, or steal the private key of another developer and use theirs instead. Therefore, keeping private keys for code signing is of utmost importance for companies that produce signed programs.


<li><a href="#">While we can use many tools and methods to attempt to statically analyze a file, often all we really need to do is open it. Since you do not know whether a document contains an Office or Reader exploit, or simply contains a malicious link or macro that would need to be run, there's no easy way to immediately know whether it is safe to view it or not. Given this condition, the best solution is to assume the document would immediately infect you and instead submit it to a malware detonation engine that will produce a screenshot or use a virtual machine that can be reverted after opening the file. If you do not have an on-premise malware detonation system like Cuckoo Sandbox, assuming you aren't worried about submitting the document for the public to see (which is a BIG if), you can use free web-based tools like malwr.com or hybrid-analysis.com.


<li><a href="#">There are a couple of easy methods to determine if a script is likely malicious right from the start. To make evil scripts look benign to automated analysis, many times the content will be as minimal as possible containing nothing more than the command to download a file or additional script code from a URL and run it. Crafting the attachment this way means there's little for file scanning tools to key off on.
</div>





<script>
function myFunction() {
    var input, filter, ul, li, a, i, txtValue;
    input = document.getElementById("myInput");
    filter = input.value.toUpperCase();
    ul = document.getElementById("myUL");
    li = ul.getElementsByTagName("li");
    for (i = 0; i < li.length; i++) {
        a = li[i].getElementsByTagName("a")[0];
        txtValue = a.textContent || a.innerText;
        if (txtValue.toUpperCase().indexOf(filter) > -1) {
            li[i].style.display = "";
        } else {
            li[i].style.display = "none";
        }
    }
}
</script>

<script src="hilitor.js"></script>
<script>
var myHilitor = new Hilitor("content"); // id of the element to parse
// myHilitor.setBreakRegExp(new RegExp('[^\\w\' -]+', "g")); // expanded to include spaces
myHilitor.apply();
</script>


<script>

  window.addEventListener("DOMContentLoaded", function(e) {
    var myHilitor2 = new Hilitor("playground");
    myHilitor2.setMatchType("left");
    document.getElementById("keywords").addEventListener("keyup", function(e) {
      myHilitor2.apply(this.value);
    }, false);
  }, false);

</script>

<script>
document.addEventListener("contextmenu", function(event){
event.preventDefault();
}, false);
</script>




</html>